"""\
–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤.

–°–æ–¥–µ—Ä–∂–∏—Ç: —Å—Ç–∏—Ö–∏–π–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫ –±–µ–∑ —Ç—Ä–æ–µ—Ç–æ—á–∏–π –∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é
–Ω–∞–∑–≤–∞–Ω–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤/—Ç–∏–ø–æ–≤.
"""

from __future__ import annotations

import re
from typing import Optional


_SENTENCE_ENDINGS = (".", "!", "?")
_SENTENCE_SPLIT_REGEX = re.compile(r"(?<=[.!?])\s+")


_PATTERN_TITLE_TRANSLATIONS = {
    "depression": "–î–µ–ø—Ä–µ—Å—Å–∏—è",
    "burnout": "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –≤—ã–≥–æ—Ä–∞–Ω–∏–µ",
    "perfectionism": "–ü–µ—Ä—Ñ–µ–∫—Ü–∏–æ–Ω–∏–∑–º",
    "imposter syndrome": "–°–∏–Ω–¥—Ä–æ–º —Å–∞–º–æ–∑–≤–∞–Ω—Ü–∞",
    "financial anxiety": "–§–∏–Ω–∞–Ω—Å–æ–≤–∞—è —Ç—Ä–µ–≤–æ–≥–∞",
    "relationship anxiety": "–¢—Ä–µ–≤–æ–≥–∞ –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö",
    "people pleasing": "–°—Ç—Ä–∞—Ö –æ—Ç–∫–∞–∑–∞—Ç—å",
    "abandonment fear": "–°—Ç—Ä–∞—Ö –±—ã—Ç—å –±—Ä–æ—à–µ–Ω–Ω—ã–º",
    "loneliness loop": "–ü–µ—Ç–ª—è –æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–∞",
    "fear of failure": "–°—Ç—Ä–∞—Ö –Ω–µ—É–¥–∞—á–∏",
    "self criticism": "–°–∞–º–æ–∫—Ä–∏—Ç–∏–∫–∞",
    "self-criticism": "–°–∞–º–æ–∫—Ä–∏—Ç–∏–∫–∞",
    "social anxiety": "–°–æ—Ü–∏–∞–ª—å–Ω–∞—è —Ç—Ä–µ–≤–æ–∂–Ω–æ—Å—Ç—å",
    "confidence gap": "–ü—Ä–æ–≤–∞–ª –≤ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏",
    "control issues": "–ü–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å –≤—Å—ë –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å",
    "avoidant coping": "–ò–∑–±–µ–≥–∞—é—â–µ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ",
    "emotional numbness": "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –æ–Ω–µ–º–µ–Ω–∏–µ",
    "sleepless stress": "–ë–µ—Å—Å–æ–Ω–Ω—ã–π —Å—Ç—Ä–µ—Å—Å",
    "parent wound": "–†–æ–¥–∏—Ç–µ–ª—å—Å–∫–∞—è —Ç—Ä–∞–≤–º–∞",
    "money guilt": "–í–∏–Ω–∞ –∑–∞ –¥–µ–Ω—å–≥–∏",
    "overthinking": "–†—É–º–∏–Ω–∞—Ü–∏—è",
    "panic spikes": "–ü—Ä–∏—Å—Ç—É–ø—ã –ø–∞–Ω–∏–∫–∏",
    "grief loop": "–¶–∏–∫–ª –ø—Ä–æ–∂–∏–≤–∞–Ω–∏–µ –≥–æ—Ä—è",
    "trust issues": "–ù–µ–¥–æ–≤–µ—Ä–∏–µ",
    "fear of success": "–°—Ç—Ä–∞—Ö —É—Å–ø–µ—Ö–∞",
    "people-pleasing": "–°—Ç—Ä–∞—Ö –æ—Ç–∫–∞–∑–∞—Ç—å",
    "procrastination": "–ü—Ä–æ–∫—Ä–∞—Å—Ç–∏–Ω–∞—Ü–∏—è",
    "hyper-independence": "–ì–∏–ø–µ—Ä–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å",
    "commitment anxiety": "–°—Ç—Ä–∞—Ö –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤",
    "emotional walls": "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å—Ç–µ–Ω—ã",
    "people pleasing loop": "–ü–µ—Ç–ª—è —É–≥–æ–∂–¥–µ–Ω–∏—è",
    "scarcity mindset": "–°—Ü–µ–Ω–∞—Ä–∏–π –Ω–µ—Ö–≤–∞—Ç–∫–∏",
    "financial guilt": "–í–∏–Ω–∞ –∑–∞ –¥–µ–Ω—å–≥–∏",
    "control loop": "–ö–æ–Ω—Ç—Ä–æ–ª—å –≤–æ –≤—Ä–µ–¥ —Å–µ–±–µ",
    "fear of intimacy": "–°—Ç—Ä–∞—Ö –±–ª–∏–∑–æ—Å—Ç–∏",
    "avoidant attachment": "–ò–∑–±–µ–≥–∞—é—â–∞—è –ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å",
    "anxious attachment": "–¢—Ä–µ–≤–æ–∂–Ω–∞—è –ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å",
    "imposter loop": "–ü–µ—Ç–ª—è —Å–∞–º–æ–∑–≤–∞–Ω—Ü–∞",
    "perfection loop": "–ü–µ—Ç–ª—è –ø–µ—Ä—Ñ–µ–∫—Ü–∏–æ–Ω–∏–∑–º–∞",
    "perfection spiral": "–°–ø–∏—Ä–∞–ª—å –ø–µ—Ä—Ñ–µ–∫—Ü–∏–æ–Ω–∏–∑–º–∞",
}


_PATTERN_TYPE_TRANSLATIONS = {
    "behavioral": "–ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–π",
    "emotional": "—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π",
    "cognitive": "–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π",
}


_TOPIC_EMOJI_MAP = {
    "relationships": "ü§ç",
    "money": "üí∏",
    "purpose": "üåø",
    "confidence": "‚òÅÔ∏è",
    "fears": "üß©",
    "sleep": "üåô",
    "dreams": "üåô",
    "stress": "‚òÅÔ∏è",
    "self": "üß©",
    "work": "üß©",
    "chat": "üí¨",
    "communication": "üí¨",
    "practices": "ü™∑",
    "video": "üé•",
}


def safe_shorten(text: Optional[str], limit: int = 160) -> str:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç, –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π –±–µ–∑ –æ–±—Ä—ã–≤–æ–≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ —Ç—Ä–æ–µ—Ç–æ—á–∏–π.
    
    –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç, –Ω–æ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç * 1.5, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ–≥–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é,
    —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –æ–±—Ä—ã–≤—ã –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ —Ñ—Ä–∞–∑.
    """

    if not text:
        return ""

    normalized = " ".join(text.strip().split())
    
    # –ú—è–≥–∫–∏–π –ª–∏–º–∏—Ç: –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç –≤ 1.5 —Ä–∞–∑–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é
    soft_limit = int(limit * 1.5)
    if len(normalized) <= soft_limit:
        return normalized
    
    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, –ø—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç—Ä–æ–≥–∏–π –ª–∏–º–∏—Ç —Å –æ–±—Ä–µ–∑–∫–æ–π –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
    sentences = _SENTENCE_SPLIT_REGEX.split(normalized)
    
    # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏ –æ–Ω–æ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –º—è–≥–∫–∏–π –ª–∏–º–∏—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é
    non_empty_sentences = [s.strip() for s in sentences if s.strip()]
    if len(non_empty_sentences) == 1 and len(non_empty_sentences[0]) <= soft_limit:
        return normalized

    collected: list[str] = []
    current_length = 0
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue

        proposed_length = current_length + (1 if collected else 0) + len(sentence)
        if proposed_length <= limit:
            collected.append(sentence)
            current_length = proposed_length
        else:
            # –ï—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è, –Ω–æ –æ–Ω–æ –æ–¥–Ω–æ –∏ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –º—è–≥–∫–∏–π –ª–∏–º–∏—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ
            if not collected and len(sentence) <= soft_limit:
                return sentence
            break

    if collected:
        result = " ".join(collected).strip()
        # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –∑–Ω–∞–∫–æ–º, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        if result.endswith(_SENTENCE_ENDINGS):
            return result

        # –ü–æ–ø—Ä–æ–±—É–µ–º —É–∫–æ—Ä–æ—Ç–∏—Ç—å –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–Ω–∞–∫–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–∏
        for index in range(len(result) - 1, -1, -1):
            if result[index] in _SENTENCE_ENDINGS:
                candidate = result[: index + 1].strip()
                if candidate:
                    return candidate

    cutoff = normalized[:limit].rstrip()

    for index in range(len(cutoff) - 1, -1, -1):
        if cutoff[index] in _SENTENCE_ENDINGS:
            candidate = cutoff[: index + 1].strip()
            if candidate:
                return candidate

    if " " in cutoff:
        candidate = cutoff.rsplit(" ", 1)[0].strip()
        if candidate:
            return candidate.rstrip(",;:-")

    return cutoff.rstrip(",;:-")


def localize_pattern_title(title: Optional[str]) -> str:
    """–ü—Ä–∏–≤–æ–¥–∏—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –∫ —Ä—É—Å—Å–∫–æ–º—É —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç—É."""

    if not title:
        return "–ü–∞—Ç—Ç–µ—Ä–Ω"

    normalized = title.strip()
    translation = _PATTERN_TITLE_TRANSLATIONS.get(normalized.lower())
    if translation:
        return translation

    return normalized


def localize_pattern_type(pattern_type: Optional[str]) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä—É—Å—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–∏–ø–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞."""

    if not pattern_type:
        return ""

    translation = _PATTERN_TYPE_TRANSLATIONS.get(pattern_type.lower())
    if translation:
        return translation

    return pattern_type


def get_topic_emoji(topic: Optional[str], fallback: str = "üß©") -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–π —ç–º–æ–¥–∑–∏ –ø–æ —Ç–µ–º–µ/–∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""

    if not topic:
        return fallback

    normalized = str(topic).lower().strip()
    return _TOPIC_EMOJI_MAP.get(normalized, fallback)


