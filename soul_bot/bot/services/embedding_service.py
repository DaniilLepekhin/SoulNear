"""
üéØ Embedding Service –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenAI Embeddings API

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è:
1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
2. Semantic similarity (–¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤)
3. –ü–æ–∏—Å–∫ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (related_patterns)
4. Semantic search –ø–æ –ø—Ä–æ—Ñ–∏–ª—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

Model: text-embedding-3-small (1536 dimensions)
Cost: $0.02 / 1M tokens (–æ—á–µ–Ω—å –¥–µ—à–µ–≤–æ!)
"""
import logging
from typing import Optional
import numpy as np
from openai import AsyncOpenAI

from config import OPENAI_API_KEY

logger = logging.getLogger(__name__)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞
client = AsyncOpenAI(api_key=OPENAI_API_KEY)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIMENSIONS = 1536
SIMILARITY_THRESHOLD_DUPLICATE = 0.85  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å = –¥—É–±–ª–∏–∫–∞—Ç
SIMILARITY_THRESHOLD_RELATED = 0.70    # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å = —Å–≤—è–∑–∞–Ω–Ω—ã–π


async def get_embedding(text: str) -> list[float]:
    """
    –ü–æ–ª—É—á–∏—Ç—å vector embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        
    Returns:
        –í–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 1536
        
    Raises:
        Exception: –ü—Ä–∏ –æ—à–∏–±–∫–µ API
    """
    try:
        response = await client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text.strip()
        )
        
        embedding = response.data[0].embedding
        
        logger.debug(f"Generated embedding for text (len={len(text)}), vector_len={len(embedding)}")
        
        return embedding
        
    except Exception as e:
        logger.error(f"Failed to generate embedding: {e}")
        raise


async def get_embeddings_batch(texts: list[str]) -> list[list[float]]:
    """
    –ü–æ–ª—É—á–∏—Ç—å embeddings –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å—Ä–∞–∑—É (batch API)
    
    Args:
        texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
        
    Returns:
        –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤
        
    Note:
        Batch API —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –¥–ª—è > 5 —Ç–µ–∫—Å—Ç–æ–≤
    """
    if not texts:
        return []
    
    try:
        # OpenAI –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç batch –¥–æ 2048 —Ç–µ–∫—Å—Ç–æ–≤
        response = await client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=[text.strip() for text in texts]
        )
        
        embeddings = [item.embedding for item in response.data]
        
        logger.debug(f"Generated {len(embeddings)} embeddings in batch")
        
        return embeddings
        
    except Exception as e:
        logger.error(f"Failed to generate batch embeddings: {e}")
        raise


def cosine_similarity(vec1: list[float], vec2: list[float]) -> float:
    """
    –í—ã—á–∏—Å–ª–∏—Ç—å cosine similarity –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏
    
    Args:
        vec1: –ü–µ—Ä–≤—ã–π –≤–µ–∫—Ç–æ—Ä
        vec2: –í—Ç–æ—Ä–æ–π –≤–µ–∫—Ç–æ—Ä
        
    Returns:
        Similarity score (0.0 - 1.0)
        1.0 = –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ, 0.0 = –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–µ
        
    Note:
        Cosine similarity = dot(A,B) / (||A|| * ||B||)
    """
    vec1_np = np.array(vec1)
    vec2_np = np.array(vec2)
    
    dot_product = np.dot(vec1_np, vec2_np)
    norm1 = np.linalg.norm(vec1_np)
    norm2 = np.linalg.norm(vec2_np)
    
    # –ó–∞—â–∏—Ç–∞ –æ—Ç division by zero
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    similarity = dot_product / (norm1 * norm2)
    
    # Clamp to [0, 1] range (–∏–Ω–æ–≥–¥–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–º–Ω–æ–≥–æ –≤—ã—à–µ –∏–∑-–∑–∞ float precision)
    return float(np.clip(similarity, 0.0, 1.0))


async def find_similar_items(
    query_embedding: list[float],
    items: list[dict],
    embedding_key: str = 'embedding',
    threshold: float = 0.0,
    top_k: int = 10
) -> list[tuple[dict, float]]:
    """
    –ù–∞–π—Ç–∏ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ items –ø–æ embedding
    
    Args:
        query_embedding: –í–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞
        items: –°–ø–∏—Å–æ–∫ items —Å embeddings
        embedding_key: –ö–ª—é—á –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ embedding –≤ item
        threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ similarity (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (item, similarity_score), –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —É–±—ã–≤–∞–Ω–∏—é similarity
        
    Example:
        >>> items = [
        ...     {"id": 1, "text": "...", "embedding": [0.1, 0.2, ...]},
        ...     {"id": 2, "text": "...", "embedding": [0.3, 0.4, ...]},
        ... ]
        >>> similar = await find_similar_items(query_vec, items, top_k=3)
        >>> for item, score in similar:
        ...     print(f"{item['text']}: {score:.2f}")
    """
    if not items:
        return []
    
    similarities = []
    
    for item in items:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º items –±–µ–∑ embedding
        if embedding_key not in item or not item[embedding_key]:
            continue
        
        # –í—ã—á–∏—Å–ª—è–µ–º similarity
        similarity = cosine_similarity(query_embedding, item[embedding_key])
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ threshold
        if similarity >= threshold:
            similarities.append((item, similarity))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é similarity
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    # –ë–µ—Ä—ë–º top_k
    return similarities[:top_k]


async def is_duplicate(
    new_text: str,
    existing_items: list[dict],
    text_key: str = 'description',
    embedding_key: str = 'embedding',
    threshold: float = SIMILARITY_THRESHOLD_DUPLICATE
) -> tuple[bool, Optional[dict], float]:
    """
    –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
    
    Args:
        new_text: –ù–æ–≤—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        existing_items: –°–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö items
        text_key: –ö–ª—é—á –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —Ç–µ–∫—Å—Ç—É (–¥–ª—è GPT-4 fallback)
        embedding_key: –ö–ª—é—á –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ embedding
        threshold: –ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–∞
        
    Returns:
        Tuple: (is_duplicate, duplicate_item, similarity_score)
        
    Example:
        >>> is_dup, dup_item, score = await is_duplicate(
        ...     "–ü—Ä–æ–∫—Ä–∞—Å—Ç–∏–Ω–∞—Ü–∏—è –ø—Ä–∏ —Å–∫—É—á–Ω–æ–π —Ä–∞–±–æ—Ç–µ",
        ...     existing_patterns
        ... )
        >>> if is_dup:
        ...     print(f"Duplicate of: {dup_item['title']} (score: {score:.2f})")
    """
    if not existing_items:
        return False, None, 0.0
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –¥–ª—è –Ω–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    new_embedding = await get_embedding(new_text)
    
    # –ò—â–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–π
    similar = await find_similar_items(
        new_embedding,
        existing_items,
        embedding_key=embedding_key,
        top_k=1
    )
    
    if not similar:
        return False, None, 0.0
    
    best_match, best_similarity = similar[0]
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç –ø–æ threshold
    is_dup = best_similarity >= threshold
    
    if is_dup:
        logger.info(
            f"Duplicate detected: '{new_text[:50]}...' ~= "
            f"'{best_match.get(text_key, 'N/A')[:50]}...' "
            f"(similarity: {best_similarity:.3f})"
        )
    
    return is_dup, best_match if is_dup else None, best_similarity


async def find_related_items(
    query_text: str,
    items: list[dict],
    embedding_key: str = 'embedding',
    threshold: float = SIMILARITY_THRESHOLD_RELATED,
    top_k: int = 5
) -> list[tuple[dict, float]]:
    """
    –ù–∞–π—Ç–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ items –ø–æ semantic similarity
    
    Args:
        query_text: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        items: –°–ø–∏—Å–æ–∫ items –¥–ª—è –ø–æ–∏—Å–∫–∞
        embedding_key: –ö–ª—é—á –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ embedding
        threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ similarity
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö items —Å scores
        
    Example:
        >>> related = await find_related_items(
        ...     "–ü—Ä–æ–±–ª–µ–º—ã —Å –º–æ—Ç–∏–≤–∞—Ü–∏–µ–π",
        ...     all_patterns,
        ...     top_k=3
        ... )
    """
    if not items:
        return []
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
    query_embedding = await get_embedding(query_text)
    
    # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ
    similar = await find_similar_items(
        query_embedding,
        items,
        embedding_key=embedding_key,
        threshold=threshold,
        top_k=top_k
    )
    
    return similar


# ==========================================
# üß™ –£–¢–ò–õ–ò–¢–´ –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø
# ==========================================

async def test_embedding_service():
    """
    –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç —Å–µ—Ä–≤–∏—Å–∞
    
    Usage:
        python -c "from bot.services.embedding_service import test_embedding_service; import asyncio; asyncio.run(test_embedding_service())"
    """
    print("üß™ Testing Embedding Service...")
    
    # Test 1: Generate single embedding
    print("\n1. Generate single embedding:")
    text = "–ü—Ä–æ–∫—Ä–∞—Å—Ç–∏–Ω–∞—Ü–∏—è –ø—Ä–∏ —Å–∫—É—á–Ω–æ–π —Ä–∞–±–æ—Ç–µ"
    embedding = await get_embedding(text)
    print(f"   Text: {text}")
    print(f"   Embedding length: {len(embedding)}")
    print(f"   First 5 values: {embedding[:5]}")
    
    # Test 2: Batch embeddings
    print("\n2. Generate batch embeddings:")
    texts = [
        "–ü—Ä–æ–∫—Ä–∞—Å—Ç–∏–Ω–∞—Ü–∏—è –ø—Ä–∏ —Å–∫—É—á–Ω–æ–π —Ä–∞–±–æ—Ç–µ",
        "–û—Ç–∫–ª–∞–¥—ã–≤–∞–µ—Ç –∑–∞–¥–∞—á–∏ –∫–æ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞ –º–æ–Ω–æ—Ç–æ–Ω–Ω–∞—è",
        "–ü—Ä–æ–±–ª–µ–º—ã —Å–æ —Å–Ω–æ–º –∏–∑-–∑–∞ —Å—Ç—Ä–µ—Å—Å–∞"
    ]
    embeddings = await get_embeddings_batch(texts)
    print(f"   Generated {len(embeddings)} embeddings")
    
    # Test 3: Cosine similarity
    print("\n3. Test cosine similarity:")
    sim1 = cosine_similarity(embeddings[0], embeddings[1])
    sim2 = cosine_similarity(embeddings[0], embeddings[2])
    print(f"   '{texts[0]}' vs '{texts[1]}': {sim1:.3f}")
    print(f"   '{texts[0]}' vs '{texts[2]}': {sim2:.3f}")
    print(f"   ‚úÖ –ü–∞—Ç—Ç–µ—Ä–Ω—ã 1-2 –±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏ ({sim1:.3f} > {sim2:.3f})")
    
    # Test 4: Duplicate detection
    print("\n4. Test duplicate detection:")
    existing = [
        {"id": 1, "text": texts[0], "embedding": embeddings[0]}
    ]
    is_dup, dup_item, score = await is_duplicate(
        "–û—Ç–∫–ª–∞–¥—ã–≤–∞–Ω–∏–µ —Ä—É—Ç–∏–Ω–Ω—ã—Ö –∑–∞–¥–∞—á",
        existing,
        text_key='text'
    )
    print(f"   Is duplicate: {is_dup}")
    print(f"   Similarity: {score:.3f}")
    
    print("\n‚úÖ All tests passed!")


if __name__ == '__main__':
    import asyncio
    asyncio.run(test_embedding_service())

