"""
üß† Pattern Analyzer Service - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:
1. Quick Analysis (–ø–æ—Å–ª–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π) - –ø–∞—Ç—Ç–µ—Ä–Ω—ã + mood
2. Deep Analysis (–ø–æ—Å–ª–µ 20 —Å–æ–æ–±—â–µ–Ω–∏–π) - –∏–Ω—Å–∞–π—Ç—ã + recommendations
3. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ embeddings
4. Learning loop (—á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç/–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)

Architecture: Moderate + Embeddings
"""
import logging
import uuid
from collections import OrderedDict
from datetime import datetime, timedelta
from typing import Optional
import json

from openai import AsyncOpenAI
from sqlalchemy import update

from config import OPENAI_API_KEY, is_feature_enabled
from bot.services import embedding_service
from bot.services.constants import (
    SIMILARITY_THRESHOLD_DUPLICATE,
    SIMILARITY_THRESHOLD_RELATED,
    QUICK_ANALYSIS_FREQUENCY,
    DEEP_ANALYSIS_FREQUENCY,
    QUICK_ANALYSIS_CONTEXT_SIZE,
    DEEP_ANALYSIS_CONTEXT_SIZE,
    QUICK_ANALYSIS_MIN_MESSAGES,
    DEEP_ANALYSIS_MIN_MESSAGES,
    MAX_EVIDENCE_PER_PATTERN,
    MAX_INSIGHTS,
    MAX_LEARNING_ITEMS,
    MODEL_ANALYSIS,
    TEMPERATURE_ANALYSIS
)
from bot.services.prompt.analysis_prompts import get_quick_analysis_prompt, get_deep_analysis_prompt
from database.repository import user_profile, conversation_history
from database.database import db
from database.models.user_profile import UserProfile
import database.repository.user as db_user

logger = logging.getLogger(__name__)

client = AsyncOpenAI(api_key=OPENAI_API_KEY)


# ==========================================
# üéØ QUICK ANALYSIS (–∫–∞–∂–¥—ã–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π)
# ==========================================

async def quick_analysis(user_id: int, assistant_type: str = 'helper'):
    """
    –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 5-10 —Å–æ–æ–±—â–µ–Ω–∏–π
    
    –í—ã—è–≤–ª—è–µ—Ç:
    - 1-2 –Ω–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    - –¢–µ–∫—É—â–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ (mood)
    - –£—Ä–æ–≤–µ–Ω—å —Å—Ç—Ä–µ—Å—Å–∞/—ç–Ω–µ—Ä–≥–∏–∏
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        assistant_type: –¢–∏–ø –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """
    if not is_feature_enabled('ENABLE_PATTERN_ANALYSIS'):
        return
    
    try:
        logger.info(f"Quick analysis for user {user_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–∏–∑ constants)
        messages = await conversation_history.get_context(
            user_id=user_id,
            assistant_type=assistant_type,
            max_messages=QUICK_ANALYSIS_CONTEXT_SIZE
        )
        
        if len(messages) < QUICK_ANALYSIS_MIN_MESSAGES:
            logger.debug("Not enough messages for analysis")
            return
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ñ–∏–ª—å
        profile = await user_profile.get_or_create(user_id)
        existing_patterns = profile.patterns.get('patterns', [])
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ GPT-4
        analysis = await _analyze_conversation_quick(messages, existing_patterns)
        
        if not analysis:
            logger.warning(f"[QUICK ANALYSIS] User {user_id}: GPT returned None")
            return
        
        # LOG: –°–∫–æ–ª—å–∫–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤–µ—Ä–Ω—É–ª GPT
        new_patterns_count = len(analysis.get('new_patterns', []))
        logger.info(f"[QUICK ANALYSIS] User {user_id}: GPT returned {new_patterns_count} new patterns")
        
        # ‚ö†Ô∏è –í–∞–ª–∏–¥–∞—Ü–∏—è: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ examples –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏–∑ —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏
        if analysis.get('new_patterns'):
            validated_patterns = [
                _validate_pattern_examples(pattern, messages)
                for pattern in analysis['new_patterns']
            ]
            analysis['new_patterns'] = validated_patterns
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã (—Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π)
        if analysis.get('new_patterns'):
            await _add_patterns_with_dedup(user_id, analysis['new_patterns'], existing_patterns)
            # LOG: –°–∫–æ–ª—å–∫–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ—Å–ª–µ –º–µ—Ä–¥–∂–∞
            updated_profile = await user_profile.get_or_create(user_id)
            total_patterns = len(updated_profile.patterns.get('patterns', []))
            logger.info(f"[QUICK ANALYSIS] User {user_id}: Total patterns after merge: {total_patterns}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º emotional state
        if analysis.get('mood'):
            mood_data = analysis['mood'].copy()
            
            # ‚ö†Ô∏è –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ stress level: –Ω–∞—à–∞ heuristic vs GPT
            # –ï—Å–ª–∏ –Ω–∞—à–∞ —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞—Ö–æ–¥–∏—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π —Å—Ç—Ä–µ—Å—Å - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
            updated_profile = await user_profile.get_or_create(user_id)
            all_patterns = updated_profile.patterns.get('patterns', [])
            calculated_stress = _calculate_stress_level(all_patterns, messages)
            gpt_stress = mood_data.get('stress_level', 'medium')
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: critical > high > medium > low
            stress_priority = {'critical': 4, 'high': 3, 'medium': 2, 'low': 1}
            if stress_priority.get(calculated_stress, 0) > stress_priority.get(gpt_stress, 0):
                logger.info(
                    f"‚ö†Ô∏è Stress level override: GPT={gpt_stress}, "
                    f"calculated={calculated_stress}. Using calculated value."
                )
                mood_data['stress_level'] = calculated_stress
            
            await _update_emotional_state(user_id, mood_data)
        
        logger.info(f"Quick analysis complete: {len(analysis.get('new_patterns', []))} patterns, mood={analysis.get('mood', {}).get('current_mood')}")
        
    except Exception as e:
        logger.error(f"Quick analysis failed: {e}")


async def _analyze_conversation_quick(
    messages: list[dict],
    existing_patterns: list[dict]
) -> Optional[dict]:
    """
    GPT-4 –∞–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–∞ (quick version)
    """
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞
    conversation_text = "\n".join([
        f"{msg['role']}: {msg['content']}" 
        for msg in messages[-10:]
    ])
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä existing patterns (GPT-4 –∫–æ–Ω—Ç–µ–∫—Å—Ç)
    existing_summaries = [
        f"- {p['title']}" 
        for p in existing_patterns[:10]
    ]
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –º–æ–¥—É–ª—è
    prompt = get_quick_analysis_prompt(conversation_text, existing_summaries)
    
    try:
        response = await client.chat.completions.create(
            model="gpt-4o-mini",  # –î–µ—à–µ–≤–ª–µ –¥–ª—è quick analysis
            messages=[
                {"role": "system", "content": "You are an expert psychologist analyzing conversation patterns."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.3
        )
        
        result = json.loads(response.choices[0].message.content)
        return result
        
    except Exception as e:
        logger.error(f"GPT-4 analysis failed: {e}")
        return None


# ==========================================
# üîç DEEP ANALYSIS (–∫–∞–∂–¥—ã–µ 20 —Å–æ–æ–±—â–µ–Ω–∏–π)
# ==========================================

async def deep_analysis(user_id: int, assistant_type: str = 'helper'):
    """
    –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤
    
    –í—ã—è–≤–ª—è–µ—Ç:
    - –°–≤—è–∑–∏ –º–µ–∂–¥—É –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
    - –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
    - –ò–Ω—Å–∞–π—Ç—ã —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
    - –û–±–Ω–æ–≤–ª—è–µ—Ç learning preferences
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        assistant_type: –¢–∏–ø –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """
    if not is_feature_enabled('ENABLE_PATTERN_ANALYSIS'):
        return
    
    try:
        logger.info(f"Deep analysis for user {user_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–∏–∑ constants)
        messages = await conversation_history.get_context(
            user_id=user_id,
            assistant_type=assistant_type,
            max_messages=DEEP_ANALYSIS_CONTEXT_SIZE
        )
        
        if len(messages) < DEEP_ANALYSIS_MIN_MESSAGES:
            logger.debug("Not enough messages for deep analysis")
            return
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ñ–∏–ª—å
        profile = await user_profile.get_or_create(user_id)
        existing_patterns = profile.patterns.get('patterns', [])
        existing_insights = profile.insights.get('insights', [])
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ GPT-4
        analysis = await _analyze_conversation_deep(messages, existing_patterns, existing_insights)
        
        if not analysis:
            return
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω—Å–∞–π—Ç—ã
        if analysis.get('insights'):
            await _add_insights(user_id, analysis['insights'], existing_patterns)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º related patterns
        if existing_patterns:
            await _update_related_patterns(user_id, existing_patterns)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º learning preferences
        if analysis.get('learning'):
            await _update_learning_preferences(user_id, analysis['learning'])
        
        logger.info(f"Deep analysis complete: {len(analysis.get('insights', []))} insights generated")
        
    except Exception as e:
        logger.error(f"Deep analysis failed: {e}")


async def _analyze_conversation_deep(
    messages: list[dict],
    existing_patterns: list[dict],
    existing_insights: list[dict]
) -> Optional[dict]:
    """
    GPT-4 –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ (insights + recommendations)
    """
    conversation_text = "\n".join([
        f"{msg['role']}: {msg['content']}" 
        for msg in messages[-30:]
    ])
    
    patterns_summary = "\n".join([
        f"- [{p['type']}] {p['title']}: {p['description']} (occurs: {p.get('occurrences', 1)}x)"
        for p in existing_patterns[:15]
    ])
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –º–æ–¥—É–ª—è
    prompt = get_deep_analysis_prompt(conversation_text, patterns_summary)
    
    try:
        response = await client.chat.completions.create(
            model="gpt-4o",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—É—é –≤–µ—Ä—Å–∏—é –¥–ª—è deep analysis
            messages=[
                {"role": "system", "content": "You are an expert psychologist providing deep insights."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.4
        )
        
        result = json.loads(response.choices[0].message.content)
        return result
        
    except Exception as e:
        logger.error(f"GPT-4 deep analysis failed: {e}")
        return None


# ==========================================
# üîÑ –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø –ò –ú–ï–†–î–ñ –ü–ê–¢–¢–ï–†–ù–û–í
# ==========================================

def _validate_pattern_examples(pattern: dict, recent_messages: list[dict]) -> dict:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è —á—Ç–æ examples –≤ –ø–∞—Ç—Ç–µ—Ä–Ω–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö
    
    Args:
        pattern: –ü–∞—Ç—Ç–µ—Ä–Ω —Å –ø–æ–ª–µ–º 'evidence' (examples)
        recent_messages: –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
    Returns:
        –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω (—É–¥–∞–ª—è–µ—Ç –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ examples)
    """
    evidence = pattern.get('evidence', [])
    if not evidence:
        return pattern
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—Ç–æ–ª—å–∫–æ user, –Ω–µ assistant)
    user_messages_text = ' '.join([
        msg.get('content', '').lower()
        for msg in recent_messages
        if msg.get('role') == 'user'
    ])
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º evidence: –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ —á—Ç–æ –µ—Å—Ç—å –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö
    validated_evidence = []
    for example in evidence:
        example_lower = example.lower().strip()
        
        # –ï—Å–ª–∏ –ø—Ä–∏–º–µ—Ä –∫–æ—Ä–æ—Ç–∫–∏–π (< 5 —Å–∏–º–≤–æ–ª–æ–≤) - —Å–∫–∏–ø–∞–µ–º (—Å–ª–∏—à–∫–æ–º –æ–±—â–∏–π)
        if len(example_lower) < 5:
            logger.debug(f"‚ö†Ô∏è Example too short, skipping: '{example}'")
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø—Ä–∏–º–µ—Ä –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö (–∏–ª–∏ –µ–≥–æ —á–∞—Å—Ç—å >= 70%)
        if example_lower in user_messages_text:
            validated_evidence.append(example)
        else:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—Ö–æ—Ç—è –±—ã 70% —Å–ª–æ–≤)
            example_words = set(example_lower.split())
            if len(example_words) > 0:
                # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –∏–∑ example –µ—Å—Ç—å –≤ messages
                matched_words = sum(1 for word in example_words if word in user_messages_text)
                match_ratio = matched_words / len(example_words)
                
                if match_ratio >= 0.7:  # 70% —Å–ª–æ–≤ —Å–æ–≤–ø–∞–ª–æ
                    validated_evidence.append(example)
                    logger.debug(f"‚úÖ Partial match ({match_ratio:.0%}): '{example}'")
                else:
                    logger.warning(
                        f"‚ùå Example NOT found in messages (match: {match_ratio:.0%}): '{example}'. "
                        f"Pattern: {pattern.get('title')}"
                    )
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω
    pattern['evidence'] = validated_evidence
    pattern['validated_at'] = datetime.now().isoformat()
    
    # –ï—Å–ª–∏ –≤—Å–µ examples –±—ã–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã - –ø–æ–Ω–∏–∂–∞–µ–º confidence
    if len(evidence) > 0 and len(validated_evidence) == 0:
        old_confidence = pattern.get('confidence', 0.7)
        pattern['confidence'] = max(0.3, old_confidence * 0.5)
        logger.warning(
            f"‚ö†Ô∏è All examples invalid for pattern '{pattern.get('title')}'. "
            f"Confidence lowered: {old_confidence:.2f} ‚Üí {pattern['confidence']:.2f}"
        )
    
    return pattern


async def _add_patterns_with_dedup(
    user_id: int,
    new_patterns: list[dict],
    existing_patterns: list[dict]
):
    """
    –î–æ–±–∞–≤–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã —á–µ—Ä–µ–∑ embeddings
    
    –î–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω—ã–π –º–µ—Ä–¥–∂:
    1. Keyword match (exact title) ‚Üí —Ñ–æ—Ä—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ä–¥–∂ (100% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
    2. Semantic similarity (embedding > threshold) ‚Üí –æ–±—ã—á–Ω—ã–π –º–µ—Ä–¥–∂
    """
    for new_pattern in new_patterns:
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è embedding
            pattern_text = f"{new_pattern['title']} {new_pattern['description']}"
            
            # FACTOR 1: Keyword match (exact title)
            keyword_match = None
            for existing in existing_patterns:
                if existing['title'].lower().strip() == new_pattern['title'].lower().strip():
                    keyword_match = existing
                    break
            
            # FACTOR 2: Semantic similarity
            is_dup, duplicate, similarity = await embedding_service.is_duplicate(
                pattern_text,
                existing_patterns,
                text_key='description',
                threshold=SIMILARITY_THRESHOLD_DUPLICATE
            )
            
            # –î–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ: keyword match > semantic similarity
            if keyword_match:
                # FORCED MERGE –ø–æ keyword (100% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
                duplicate = keyword_match
                is_dup = True
                similarity = 1.0
                logger.info(f"üîë KEYWORD MATCH: '{new_pattern['title']}' ‚Üí forced merge")
            
            if is_dup:
                # –ú–µ—Ä–¥–∂–∏–º —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º
                old_occurrences = duplicate.get('occurrences', 1)
                duplicate['occurrences'] = old_occurrences + 1
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ evidence (–±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –º–∞–∫—Å–∏–º—É–º 10)
                existing_evidence = set(duplicate.get('evidence', []))
                new_evidence = [e for e in new_pattern.get('evidence', []) if e not in existing_evidence]
                duplicate['evidence'].extend(new_evidence)
                duplicate['evidence'] = duplicate['evidence'][-10:]  # Limit to last 10
                
                duplicate['last_detected'] = datetime.now().isoformat()
                duplicate['confidence'] = max(duplicate['confidence'], new_pattern.get('confidence', 0.7))
                
                logger.info(f"‚úÖ MERGED: '{new_pattern['title']}' ‚Üí '{duplicate['title']}' | similarity: {similarity:.2f} | occurrences: {old_occurrences} ‚Üí {duplicate['occurrences']}")
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π
                new_pattern['id'] = str(uuid.uuid4())
                new_pattern['first_detected'] = datetime.now().isoformat()
                new_pattern['last_detected'] = datetime.now().isoformat()
                new_pattern['occurrences'] = 1
                new_pattern['related_patterns'] = []
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding
                new_pattern['embedding'] = await embedding_service.get_embedding(pattern_text)
                
                existing_patterns.append(new_pattern)
                
                logger.info(f"Added new pattern: {new_pattern['title']}")
        
        except Exception as e:
            logger.error(f"Failed to process pattern: {e}")
            continue
    
    # Limit: –º–∞–∫—Å–∏–º—É–º 20 –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (—É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å –Ω–∏–∑–∫–∏–º confidence)
    if len(existing_patterns) > 20:
        existing_patterns.sort(
            key=lambda x: (x.get('confidence', 0.5), x.get('occurrences', 1)),
            reverse=True
        )
        existing_patterns = existing_patterns[:20]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    await user_profile.update_patterns(user_id, existing_patterns)


async def _update_related_patterns(user_id: int, patterns: list[dict]):
    """
    –û–±–Ω–æ–≤–∏—Ç—å related_patterns —á–µ—Ä–µ–∑ semantic similarity
    """
    for i, pattern in enumerate(patterns):
        if 'embedding' not in pattern or not pattern['embedding']:
            continue
        
        # –ù–∞—Ö–æ–¥–∏–º 3 –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞ (–∏—Å–∫–ª—é—á–∞—è —Å–µ–±—è)
        other_patterns = [p for j, p in enumerate(patterns) if j != i]
        
        related = await embedding_service.find_similar_items(
            pattern['embedding'],
            other_patterns,
            threshold=SIMILARITY_THRESHOLD_RELATED,
            top_k=3
        )
        
        pattern['related_patterns'] = [p['id'] for p, _ in related]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    await user_profile.update_patterns(user_id, patterns)


# ==========================================
# üí° –ò–ù–°–ê–ô–¢–´
# ==========================================

async def _add_insights(user_id: int, new_insights: list[dict], existing_patterns: list[dict]):
    """
    –î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Å–∞–π—Ç—ã (—Å —Å–≤—è–∑—å—é –∫ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º)
    """
    profile = await user_profile.get_or_create(user_id)
    existing_insights = profile.insights.get('insights', [])
    
    for insight in new_insights:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID
        insight['id'] = str(uuid.uuid4())
        insight['created_at'] = datetime.now().isoformat()
        insight['last_updated'] = datetime.now().isoformat()
        
        # –°–≤—è–∑—ã–≤–∞–µ–º —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ (–ø–æ title)
        derived_titles = insight.get('derived_from_pattern_titles', [])
        insight['derived_from'] = [
            p['id'] for p in existing_patterns
            if p['title'] in derived_titles
        ]
        
        existing_insights.append(insight)
    
    # Limit: –º–∞–∫—Å–∏–º—É–º 10 –∏–Ω—Å–∞–π—Ç–æ–≤
    if len(existing_insights) > 10:
        existing_insights = existing_insights[-10:]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    await user_profile.update_insights(user_id, existing_insights)


# ==========================================
# üòä EMOTIONAL STATE
# ==========================================

def _calculate_stress_level(patterns: list[dict], recent_messages: list[dict]) -> str:
    """
    –†–∞—Å—Å—á–∏—Ç–∞—Ç—å —É—Ä–æ–≤–µ–Ω—å —Å—Ç—Ä–µ—Å—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ —Å–æ–æ–±—â–µ–Ω–∏–π
    
    Args:
        patterns: –°–ø–∏—Å–æ–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        recent_messages: –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        
    Returns:
        'critical' | 'high' | 'medium' | 'low'
    """
    stress_score = 0
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä–µ—Å—Å-–ø–∞—Ç—Ç–µ—Ä–Ω—ã
    stress_patterns = ['burnout', 'exhaustion', '–≤—ã–≥–æ—Ä–∞–Ω–∏–µ', '–ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞']
    critical_patterns = ['panic', '–ø–∞–Ω–∏–∫–∞', 'despair', '–æ—Ç—á–∞—è–Ω–∏–µ']
    
    for pattern in patterns:
        title_lower = pattern.get('title', '').lower()
        category_lower = pattern.get('category', '').lower()
        frequency = pattern.get('occurrences', 1)
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if any(kw in title_lower or kw in category_lower for kw in critical_patterns):
            stress_score += 4 * frequency
        
        # Burnout —Å –≤—ã—Å–æ–∫–æ–π —á–∞—Å—Ç–æ—Ç–æ–π
        if any(kw in title_lower or kw in category_lower for kw in stress_patterns):
            stress_score += 3 * frequency
        
        # –°—Ç—Ä–∞—Ö –Ω–µ—É–¥–∞—á–∏, —Å–∏–Ω–¥—Ä–æ–º —Å–∞–º–æ–∑–≤–∞–Ω—Ü–∞
        if any(kw in title_lower for kw in ['—Å—Ç—Ä–∞—Ö', '—Å–∞–º–æ–∑–≤–∞–Ω–µ—Ü', 'fear', 'imposter']):
            stress_score += 2 * frequency
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –Ω–∞ burnout keywords
    burnout_keywords = [
        '—Ä–∞–±–æ—Ç–∞—é –ø–æ', '—Ä–∞–±–æ—Ç–∞—é 12', '–Ω–µ—Ç —Å–∏–ª', '–Ω–µ –º–æ–≥—É –¥—É–º–∞—Ç—å',
        '–∑–∞–±—ã–ª', '—Ö–æ—á–µ—Ç—Å—è –≤—Å—ë –±—Ä–æ—Å–∏—Ç—å', '–±–æ–ª—å—à–µ –Ω–µ –º–æ–≥—É',
        '–±–µ–∑ –≤—ã—Ö–æ–¥–Ω—ã—Ö', '–∫–∞–∂–¥—ã–π –¥–µ–Ω—å —Ä–∞–±–æ—Ç'
    ]
    
    recent_text = ' '.join([msg.get('content', '').lower() for msg in recent_messages[-10:]])
    
    burnout_count = sum(1 for kw in burnout_keywords if kw in recent_text)
    stress_score += burnout_count * 2
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    if stress_score >= 15:
        return 'critical'
    elif stress_score >= 8:
        return 'high'
    elif stress_score >= 3:
        return 'medium'
    else:
        return 'low'


def _detect_contradictions(emotional_state: dict, patterns: list[dict]) -> dict:
    """
    –û–±–Ω–∞—Ä—É–∂–∏—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –º–µ–∂–¥—É emotional_state –∏ patterns
    
    Args:
        emotional_state: –¢–µ–∫—É—â–µ–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        patterns: –°–ø–∏—Å–æ–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        
    Returns:
        –°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å —Ñ–ª–∞–≥–æ–º 'auto_corrected'
    """
    corrected_state = emotional_state.copy()
    auto_corrections = []
    
    stress_level = emotional_state.get('stress_level', 'medium')
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É burnout/stress –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    stress_frequency = sum(
        p.get('occurrences', 1) 
        for p in patterns 
        if any(kw in p.get('title', '').lower() or kw in p.get('category', '').lower()
               for kw in ['burnout', '–≤—ã–≥–æ—Ä–∞–Ω–∏–µ', 'stress', '—Å—Ç—Ä–µ—Å—Å', 'exhaustion'])
    )
    
    # –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ: stress_level='low' –Ω–æ –≤—ã—Å–æ–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞ stress patterns
    if stress_level in ['low', 'medium'] and stress_frequency >= 5:
        logger.warning(
            f"‚ö†Ô∏è Contradiction detected: stress_level={stress_level}, "
            f"but burnout patterns frequency={stress_frequency}. Auto-correcting to 'high'."
        )
        corrected_state['stress_level'] = 'high'
        auto_corrections.append({
            'field': 'stress_level',
            'old_value': stress_level,
            'new_value': 'high',
            'reason': f'High burnout pattern frequency ({stress_frequency})'
        })
    
    # –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ: energy_level='high' –Ω–æ –º–Ω–æ–≥–æ —É—Å—Ç–∞–ª–æ—Å—Ç–∏ –≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö
    energy_level = emotional_state.get('energy_level', 'medium')
    fatigue_frequency = sum(
        p.get('occurrences', 1)
        for p in patterns
        if any(kw in p.get('title', '').lower()
               for kw in ['—É—Å—Ç–∞–ª–æ—Å—Ç—å', '–Ω–µ—Ç —Å–∏–ª', 'exhaustion', 'fatigue', 'burnout'])
    )
    
    if energy_level == 'high' and fatigue_frequency >= 3:
        logger.warning(
            f"‚ö†Ô∏è Contradiction detected: energy_level={energy_level}, "
            f"but fatigue patterns frequency={fatigue_frequency}. Auto-correcting to 'low'."
        )
        corrected_state['energy_level'] = 'low'
        auto_corrections.append({
            'field': 'energy_level',
            'old_value': energy_level,
            'new_value': 'low',
            'reason': f'High fatigue pattern frequency ({fatigue_frequency})'
        })
    
    if auto_corrections:
        corrected_state['auto_corrections'] = auto_corrections
        corrected_state['last_correction'] = datetime.now().isoformat()
    
    return corrected_state


async def _update_emotional_state(user_id: int, mood_data: dict):
    """
    –û–±–Ω–æ–≤–∏—Ç—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    """
    profile = await user_profile.get_or_create(user_id)
    emotional_state = profile.emotional_state
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    emotional_state['current_mood'] = mood_data.get('current_mood', 'neutral')
    emotional_state['stress_level'] = mood_data.get('stress_level', 'medium')
    emotional_state['energy_level'] = mood_data.get('energy_level', 'medium')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é (limit: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π)
    mood_history = emotional_state.get('mood_history', [])
    today = datetime.now().date().isoformat()
    
    # –£–¥–∞–ª—è–µ–º —Å–µ–≥–æ–¥–Ω—è—à–Ω—é—é –∑–∞–ø–∏—Å—å –µ—Å–ª–∏ –µ—Å—Ç—å (–±—É–¥–µ–º –æ–±–Ω–æ–≤–ª—è—Ç—å)
    mood_history = [m for m in mood_history if m.get('date') != today]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é
    mood_history.append({
        'date': today,
        'mood': mood_data.get('current_mood'),
        'triggers': mood_data.get('triggers', [])
    })
    
    # Limit: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –∑–∞–ø–∏—Å–µ–π
    emotional_state['mood_history'] = mood_history[-30:]
    
    # ‚ö†Ô∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π: –µ—Å–ª–∏ GPT —Å–∫–∞–∑–∞–ª "stress=low", –Ω–æ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≥–æ–≤–æ—Ä—è—Ç "burnout"
    patterns = profile.patterns.get('patterns', [])
    emotional_state = _detect_contradictions(emotional_state, patterns)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    async with db() as session:
        await session.execute(
            update(UserProfile)
            .where(UserProfile.user_id == user_id)
            .values(
                emotional_state=emotional_state,
                updated_at=datetime.now()
            )
        )
        await session.commit()


# ==========================================
# üéì LEARNING PREFERENCES
# ==========================================

async def _update_learning_preferences(user_id: int, learning_data: dict):
    """
    –û–±–Ω–æ–≤–∏—Ç—å learning preferences (—á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç/–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç OrderedDict –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞ (–Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –∫–æ–Ω–µ—Ü).
    –≠—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è UI - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–µ—Ä–≤—ã–º–∏.
    """
    profile = await user_profile.get_or_create(user_id)
    learning_prefs = profile.learning_preferences
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º OrderedDict –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞ (–Ω–æ–≤—ã–µ –≤ –∫–æ–Ω–µ—Ü)
    # –ö–ª—é—á–∏ = items, –∑–Ω–∞—á–µ–Ω–∏—è = None (–Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–∏)
    works_well = OrderedDict.fromkeys(learning_prefs.get('works_well', []))
    doesnt_work = OrderedDict.fromkeys(learning_prefs.get('doesnt_work', []))
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (–¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è)
    for item in learning_data.get('works_well', []):
        works_well[item] = None
    for item in learning_data.get('doesnt_work', []):
        doesnt_work[item] = None
    
    # Limit: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 (—Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ)
    learning_prefs['works_well'] = list(works_well.keys())[-MAX_LEARNING_ITEMS:]
    learning_prefs['doesnt_work'] = list(doesnt_work.keys())[-MAX_LEARNING_ITEMS:]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    async with db() as session:
        await session.execute(
            update(UserProfile)
            .where(UserProfile.user_id == user_id)
            .values(
                learning_preferences=learning_prefs,
                updated_at=datetime.now()
            )
        )
        await session.commit()


# ==========================================
# üéØ PUBLIC API
# ==========================================

async def analyze_if_needed(user_id: int, assistant_type: str = 'helper'):
    """
    –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω—É–∂–µ–Ω –ª–∏ –∞–Ω–∞–ª–∏–∑ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    
    –¢—Ä–∏–≥–≥–µ—Ä—ã:
    - –ü–æ—Å–ª–µ 3 —Å–æ–æ–±—â–µ–Ω–∏–π ‚Üí quick analysis (—É–≤–µ–ª–∏—á–µ–Ω–æ —Å 5 –¥–ª—è —Ä–æ—Å—Ç–∞ occurrences)
    - –ü–æ—Å–ª–µ 20 —Å–æ–æ–±—â–µ–Ω–∏–π ‚Üí deep analysis
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        assistant_type: –¢–∏–ø –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """
    if not is_feature_enabled('ENABLE_PATTERN_ANALYSIS'):
        return
    
    # –°—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
    message_count = await conversation_history.count_messages(user_id, assistant_type)
    
    # Quick analysis (—á–∞—Å—Ç–æ—Ç–∞ –∏–∑ constants)
    if message_count > 0 and message_count % QUICK_ANALYSIS_FREQUENCY == 0:
        await quick_analysis(user_id, assistant_type)
    
    # Deep analysis (—á–∞—Å—Ç–æ—Ç–∞ –∏–∑ constants)
    if message_count > 0 and message_count % DEEP_ANALYSIS_FREQUENCY == 0:
        await deep_analysis(user_id, assistant_type)

