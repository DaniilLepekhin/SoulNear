"""
üß† Pattern Analyzer Service - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:
1. Quick Analysis (–ø–æ—Å–ª–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π) - –ø–∞—Ç—Ç–µ—Ä–Ω—ã + mood
2. Deep Analysis (–ø–æ—Å–ª–µ 20 —Å–æ–æ–±—â–µ–Ω–∏–π) - –∏–Ω—Å–∞–π—Ç—ã + recommendations
3. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ embeddings
4. Learning loop (—á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç/–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)

Architecture: Moderate + Embeddings
"""
import logging
import uuid
from datetime import datetime, timedelta
from typing import Optional
import json

from openai import AsyncOpenAI

from config import OPENAI_API_KEY, is_feature_enabled
from bot.services import embedding_service
from database.repository import user_profile, conversation_history
import database.repository.user as db_user

logger = logging.getLogger(__name__)

client = AsyncOpenAI(api_key=OPENAI_API_KEY)


# ==========================================
# üéØ QUICK ANALYSIS (–∫–∞–∂–¥—ã–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π)
# ==========================================

async def quick_analysis(user_id: int, assistant_type: str = 'helper'):
    """
    –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 5-10 —Å–æ–æ–±—â–µ–Ω–∏–π
    
    –í—ã—è–≤–ª—è–µ—Ç:
    - 1-2 –Ω–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    - –¢–µ–∫—É—â–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ (mood)
    - –£—Ä–æ–≤–µ–Ω—å —Å—Ç—Ä–µ—Å—Å–∞/—ç–Ω–µ—Ä–≥–∏–∏
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        assistant_type: –¢–∏–ø –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """
    if not is_feature_enabled('ENABLE_PATTERN_ANALYSIS'):
        return
    
    try:
        logger.info(f"Quick analysis for user {user_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 15 —Å–æ–æ–±—â–µ–Ω–∏–π (—É–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–π –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π)
        messages = await conversation_history.get_context(
            user_id=user_id,
            assistant_type=assistant_type,
            max_messages=15
        )
        
        if len(messages) < 4:
            logger.debug("Not enough messages for analysis")
            return
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ñ–∏–ª—å
        profile = await user_profile.get_or_create(user_id)
        existing_patterns = profile.patterns.get('patterns', [])
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ GPT-4
        analysis = await _analyze_conversation_quick(messages, existing_patterns)
        
        if not analysis:
            logger.warning(f"[QUICK ANALYSIS] User {user_id}: GPT returned None")
            return
        
        # LOG: –°–∫–æ–ª—å–∫–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤–µ—Ä–Ω—É–ª GPT
        new_patterns_count = len(analysis.get('new_patterns', []))
        logger.info(f"[QUICK ANALYSIS] User {user_id}: GPT returned {new_patterns_count} new patterns")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã (—Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π)
        if analysis.get('new_patterns'):
            await _add_patterns_with_dedup(user_id, analysis['new_patterns'], existing_patterns)
            # LOG: –°–∫–æ–ª—å–∫–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ—Å–ª–µ –º–µ—Ä–¥–∂–∞
            updated_profile = await user_profile.get_or_create(user_id)
            total_patterns = len(updated_profile.patterns.get('patterns', []))
            logger.info(f"[QUICK ANALYSIS] User {user_id}: Total patterns after merge: {total_patterns}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º emotional state
        if analysis.get('mood'):
            await _update_emotional_state(user_id, analysis['mood'])
        
        logger.info(f"Quick analysis complete: {len(analysis.get('new_patterns', []))} patterns, mood={analysis.get('mood', {}).get('current_mood')}")
        
    except Exception as e:
        logger.error(f"Quick analysis failed: {e}")


async def _analyze_conversation_quick(
    messages: list[dict],
    existing_patterns: list[dict]
) -> Optional[dict]:
    """
    GPT-4 –∞–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–∞ (quick version)
    """
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞
    conversation_text = "\n".join([
        f"{msg['role']}: {msg['content']}" 
        for msg in messages[-10:]
    ])
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä existing patterns (GPT-4 –∫–æ–Ω—Ç–µ–∫—Å—Ç)
    existing_summaries = [
        f"- {p['title']}" 
        for p in existing_patterns[:10]
    ]
    
    prompt = f"""
Analyze this conversation and extract behavioral/emotional patterns.

‚ö†Ô∏è CRITICAL: Use ESTABLISHED CLINICAL/PSYCHOLOGICAL TERMINOLOGY, create BROAD patterns.

üåê LANGUAGE RULE: ALL pattern titles MUST be in ENGLISH!
Examples: "Imposter Syndrome" (NOT "–°–∏–Ω–¥—Ä–æ–º —Å–∞–º–æ–∑–≤–∞–Ω—Ü–∞")
          "Perfectionism" (NOT "–ü–µ—Ä—Ñ–µ–∫—Ü–∏–æ–Ω–∏–∑–º")
          "Social Anxiety in Professional Settings" (NOT "–°–æ—Ü–∏–∞–ª—å–Ω–∞—è —Ç—Ä–µ–≤–æ–≥–∞")
This ensures consistent embedding similarity and proper merging!

üéØ EXPECTED PATTERNS (these are SEPARATE, don't merge them):
1. "Imposter Syndrome" - feeling inadequate, fraud, "not good enough", fear of being exposed
2. "Perfectionism" - code must be perfect, rewriting 10 times, fear of mistakes, paralysis
3. "Social Anxiety in Professional Settings" - fear asking questions, avoiding meetings/calls
4. "Negative Self-Talk" - persistent internal critical voice
5. "Fear of Failure" - avoiding tasks due to anticipated negative outcomes
6. "Procrastination Through Over-Analysis" - paralysis by analysis, overthinking

‚ö†Ô∏è NOTE: Perfectionism ‚â† Imposter Syndrome (they often co-occur but are DISTINCT patterns!)

‚úÖ GOOD pattern titles (use THESE exact terms when applicable):
"Imposter Syndrome" - feeling inadequate despite evidence of competence
"Perfectionism" - setting unrealistically high standards, fear of mistakes  
"Social Anxiety in Professional Settings" - fear of judgment/criticism at work
"Negative Self-Talk" - persistent internal critical voice
"Procrastination Through Over-Analysis" - paralysis by analysis, overthinking
"Fear of Failure" - avoiding tasks due to anticipated negative outcomes
"Catastrophic Thinking" - expecting worst-case scenarios

‚ùå BAD examples (what NOT to do - from real test data):
"–ù–µ–≥–∞—Ç–∏–≤–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ —Å–µ–±—è" ‚Üí should be "Imposter Syndrome"
"–°–∞–º–æ—Ä–∞–∑—Ä—É—à–∏—Ç–µ–ª—å–Ω—ã–µ –º—ã—Å–ª–∏" ‚Üí should be "Negative Self-Talk" 
"–°–æ—Ü–∏–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ" ‚Üí should be "Imposter Syndrome"
"–°—Ç—Ä–∞—Ö –æ—Å—É–∂–¥–µ–Ω–∏—è" ‚Üí should be "Social Anxiety in Professional Settings"
"Seeking external validation" ‚Üí part of "Imposter Syndrome"
"Difficulty with self-acceptance" ‚Üí part of "Imposter Syndrome"

üéØ MERGING RULE (CRITICAL - FIXED LOGIC):
If you see evidence of an EXISTING pattern in current conversation ‚Üí CREATE IT AGAIN with NEW evidence!
This is how we track frequency. The embeddings will auto-merge and increase occurrences.

Example: User says "I'm not good enough" again in messages 10-15
‚Üí CREATE pattern "Imposter Syndrome" again with this NEW quote as evidence
‚Üí System will merge it with existing pattern and increase occurrences: 1 ‚Üí 2
‚Üí This happens every time pattern appears ‚Üí occurrences grows!

‚ö†Ô∏è DO create same pattern multiple times if it repeats in conversation
‚ö†Ô∏è DON'T create variations (Self-doubt, Low self-worth) - use established term
‚ö†Ô∏è WHEN IN DOUBT: Choose BROADER term, but DO return it if you see it again!

CONVERSATION (last 10 messages):
{conversation_text}

EXISTING PATTERNS (DO NOT create variations of these):
{chr(10).join(existing_summaries) if existing_summaries else 'None yet'}

Tasks:
1. Find 1-2 BROAD patterns in current conversation (CREATE again if it repeats!)
2. Use ENGLISH titles with established psychology/DSM terminology
3. Detect current mood and energy level
4. If theme repeats ‚Üí CREATE pattern AGAIN with new evidence (for occurrences tracking!)

Return JSON:
{{
  "new_patterns": [
    {{
      "type": "behavioral|emotional|cognitive",
      "title": "Clinical Term (3-5 words, use established terminology)",
      "description": "Detailed psychological explanation with theory reference",
      "evidence": ["exact quote 1", "exact quote 2"],
      "tags": ["DSM-related", "clinical-psychology"],
      "frequency": "high|medium|low",
      "confidence": 0.7-1.0
    }}
  ],
  "mood": {{
    "current_mood": "slightly_down|neutral|good|energetic|stressed",
    "stress_level": "low|medium|high",
    "energy_level": "low|medium|high",
    "triggers": ["trigger1", "trigger2"]
  }}
}}

üö® FINAL CHECK before returning:
- Is this title an ESTABLISHED psychological term? (Google it if unsure)
- Is it DIFFERENT enough from existing patterns? (If similar ‚Üí return empty array)
- Would a clinical psychologist recognize this term? (If no ‚Üí rephrase)

Quality > Quantity. Empty array is better than creating near-duplicates.
"""
    
    try:
        response = await client.chat.completions.create(
            model="gpt-4o-mini",  # –î–µ—à–µ–≤–ª–µ –¥–ª—è quick analysis
            messages=[
                {"role": "system", "content": "You are an expert psychologist analyzing conversation patterns."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.3
        )
        
        result = json.loads(response.choices[0].message.content)
        return result
        
    except Exception as e:
        logger.error(f"GPT-4 analysis failed: {e}")
        return None


# ==========================================
# üîç DEEP ANALYSIS (–∫–∞–∂–¥—ã–µ 20 —Å–æ–æ–±—â–µ–Ω–∏–π)
# ==========================================

async def deep_analysis(user_id: int, assistant_type: str = 'helper'):
    """
    –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤
    
    –í—ã—è–≤–ª—è–µ—Ç:
    - –°–≤—è–∑–∏ –º–µ–∂–¥—É –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
    - –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
    - –ò–Ω—Å–∞–π—Ç—ã —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
    - –û–±–Ω–æ–≤–ª—è–µ—Ç learning preferences
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        assistant_type: –¢–∏–ø –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """
    if not is_feature_enabled('ENABLE_PATTERN_ANALYSIS'):
        return
    
    try:
        logger.info(f"Deep analysis for user {user_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 —Å–æ–æ–±—â–µ–Ω–∏–π
        messages = await conversation_history.get_context(
            user_id=user_id,
            assistant_type=assistant_type,
            max_messages=30
        )
        
        if len(messages) < 10:
            logger.debug("Not enough messages for deep analysis")
            return
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ñ–∏–ª—å
        profile = await user_profile.get_or_create(user_id)
        existing_patterns = profile.patterns.get('patterns', [])
        existing_insights = profile.insights.get('insights', [])
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ GPT-4
        analysis = await _analyze_conversation_deep(messages, existing_patterns, existing_insights)
        
        if not analysis:
            return
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω—Å–∞–π—Ç—ã
        if analysis.get('insights'):
            await _add_insights(user_id, analysis['insights'], existing_patterns)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º related patterns
        if existing_patterns:
            await _update_related_patterns(user_id, existing_patterns)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º learning preferences
        if analysis.get('learning'):
            await _update_learning_preferences(user_id, analysis['learning'])
        
        logger.info(f"Deep analysis complete: {len(analysis.get('insights', []))} insights generated")
        
    except Exception as e:
        logger.error(f"Deep analysis failed: {e}")


async def _analyze_conversation_deep(
    messages: list[dict],
    existing_patterns: list[dict],
    existing_insights: list[dict]
) -> Optional[dict]:
    """
    GPT-4 –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ (insights + recommendations)
    """
    conversation_text = "\n".join([
        f"{msg['role']}: {msg['content']}" 
        for msg in messages[-30:]
    ])
    
    patterns_summary = "\n".join([
        f"- [{p['type']}] {p['title']}: {p['description']} (occurs: {p.get('occurrences', 1)}x)"
        for p in existing_patterns[:15]
    ])
    
    prompt = f"""
Deep analysis of user's behavioral patterns and conversation history.

CONVERSATION (last 30 messages):
{conversation_text}

IDENTIFIED PATTERNS:
{patterns_summary if patterns_summary else 'No patterns yet'}

Tasks:
1. Generate 1-2 HIGH-LEVEL INSIGHTS from patterns
2. Provide actionable RECOMMENDATIONS
3. Identify what communication style WORKS WELL vs DOESN'T WORK

Return JSON:
{{
  "insights": [
    {{
      "category": "personality|behavior|emotional",
      "title": "Insight title",
      "description": "Detailed description connecting multiple patterns",
      "impact": "negative|neutral|positive",
      "recommendations": ["action1", "action2"],
      "derived_from_pattern_titles": ["pattern title 1", "pattern title 2"],
      "priority": "high|medium|low"
    }}
  ],
  "learning": {{
    "works_well": ["what works for this user"],
    "doesnt_work": ["what doesn't work"]
  }}
}}
"""
    
    try:
        response = await client.chat.completions.create(
            model="gpt-4o",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—É—é –≤–µ—Ä—Å–∏—é –¥–ª—è deep analysis
            messages=[
                {"role": "system", "content": "You are an expert psychologist providing deep insights."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.4
        )
        
        result = json.loads(response.choices[0].message.content)
        return result
        
    except Exception as e:
        logger.error(f"GPT-4 deep analysis failed: {e}")
        return None


# ==========================================
# üîÑ –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø –ò –ú–ï–†–î–ñ –ü–ê–¢–¢–ï–†–ù–û–í
# ==========================================

async def _add_patterns_with_dedup(
    user_id: int,
    new_patterns: list[dict],
    existing_patterns: list[dict]
):
    """
    –î–æ–±–∞–≤–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã —á–µ—Ä–µ–∑ embeddings
    """
    for new_pattern in new_patterns:
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è embedding
            pattern_text = f"{new_pattern['title']} {new_pattern['description']}"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç (–∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π threshold)
            from bot.services.embedding_service import SIMILARITY_THRESHOLD_DUPLICATE
            is_dup, duplicate, similarity = await embedding_service.is_duplicate(
                pattern_text,
                existing_patterns,
                text_key='description',
                threshold=SIMILARITY_THRESHOLD_DUPLICATE
            )
            
            if is_dup:
                # –ú–µ—Ä–¥–∂–∏–º —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º
                old_occurrences = duplicate.get('occurrences', 1)
                duplicate['occurrences'] = old_occurrences + 1
                duplicate['evidence'].extend(new_pattern.get('evidence', []))
                duplicate['last_detected'] = datetime.now().isoformat()
                duplicate['confidence'] = max(duplicate['confidence'], new_pattern.get('confidence', 0.7))
                
                logger.info(f"‚úÖ MERGED: '{new_pattern['title']}' ‚Üí '{duplicate['title']}' | similarity: {similarity:.2f} | occurrences: {old_occurrences} ‚Üí {duplicate['occurrences']}")
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π
                new_pattern['id'] = str(uuid.uuid4())
                new_pattern['first_detected'] = datetime.now().isoformat()
                new_pattern['last_detected'] = datetime.now().isoformat()
                new_pattern['occurrences'] = 1
                new_pattern['related_patterns'] = []
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding
                new_pattern['embedding'] = await embedding_service.get_embedding(pattern_text)
                
                existing_patterns.append(new_pattern)
                
                logger.info(f"Added new pattern: {new_pattern['title']}")
        
        except Exception as e:
            logger.error(f"Failed to process pattern: {e}")
            continue
    
    # Limit: –º–∞–∫—Å–∏–º—É–º 20 –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (—É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å –Ω–∏–∑–∫–∏–º confidence)
    if len(existing_patterns) > 20:
        existing_patterns.sort(
            key=lambda x: (x.get('confidence', 0.5), x.get('occurrences', 1)),
            reverse=True
        )
        existing_patterns = existing_patterns[:20]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    await user_profile.update_patterns(user_id, existing_patterns)


async def _update_related_patterns(user_id: int, patterns: list[dict]):
    """
    –û–±–Ω–æ–≤–∏—Ç—å related_patterns —á–µ—Ä–µ–∑ semantic similarity
    """
    for i, pattern in enumerate(patterns):
        if 'embedding' not in pattern or not pattern['embedding']:
            continue
        
        # –ù–∞—Ö–æ–¥–∏–º 3 –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞ (–∏—Å–∫–ª—é—á–∞—è —Å–µ–±—è)
        other_patterns = [p for j, p in enumerate(patterns) if j != i]
        
        from bot.services.embedding_service import SIMILARITY_THRESHOLD_RELATED
        related = await embedding_service.find_similar_items(
            pattern['embedding'],
            other_patterns,
            threshold=SIMILARITY_THRESHOLD_RELATED,
            top_k=3
        )
        
        pattern['related_patterns'] = [p['id'] for p, _ in related]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    await user_profile.update_patterns(user_id, patterns)


# ==========================================
# üí° –ò–ù–°–ê–ô–¢–´
# ==========================================

async def _add_insights(user_id: int, new_insights: list[dict], existing_patterns: list[dict]):
    """
    –î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Å–∞–π—Ç—ã (—Å —Å–≤—è–∑—å—é –∫ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º)
    """
    profile = await user_profile.get_or_create(user_id)
    existing_insights = profile.insights.get('insights', [])
    
    for insight in new_insights:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID
        insight['id'] = str(uuid.uuid4())
        insight['created_at'] = datetime.now().isoformat()
        insight['last_updated'] = datetime.now().isoformat()
        
        # –°–≤—è–∑—ã–≤–∞–µ–º —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ (–ø–æ title)
        derived_titles = insight.get('derived_from_pattern_titles', [])
        insight['derived_from'] = [
            p['id'] for p in existing_patterns
            if p['title'] in derived_titles
        ]
        
        existing_insights.append(insight)
    
    # Limit: –º–∞–∫—Å–∏–º—É–º 10 –∏–Ω—Å–∞–π—Ç–æ–≤
    if len(existing_insights) > 10:
        existing_insights = existing_insights[-10:]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    await user_profile.update_insights(user_id, existing_insights)


# ==========================================
# üòä EMOTIONAL STATE
# ==========================================

async def _update_emotional_state(user_id: int, mood_data: dict):
    """
    –û–±–Ω–æ–≤–∏—Ç—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    """
    profile = await user_profile.get_or_create(user_id)
    emotional_state = profile.emotional_state
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    emotional_state['current_mood'] = mood_data.get('current_mood', 'neutral')
    emotional_state['stress_level'] = mood_data.get('stress_level', 'medium')
    emotional_state['energy_level'] = mood_data.get('energy_level', 'medium')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é (limit: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π)
    mood_history = emotional_state.get('mood_history', [])
    today = datetime.now().date().isoformat()
    
    # –£–¥–∞–ª—è–µ–º —Å–µ–≥–æ–¥–Ω—è—à–Ω—é—é –∑–∞–ø–∏—Å—å –µ—Å–ª–∏ –µ—Å—Ç—å (–±—É–¥–µ–º –æ–±–Ω–æ–≤–ª—è—Ç—å)
    mood_history = [m for m in mood_history if m.get('date') != today]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é
    mood_history.append({
        'date': today,
        'mood': mood_data.get('current_mood'),
        'triggers': mood_data.get('triggers', [])
    })
    
    # Limit: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –∑–∞–ø–∏—Å–µ–π
    emotional_state['mood_history'] = mood_history[-30:]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    from database.database import db
    from sqlalchemy import update
    from database.models.user_profile import UserProfile
    
    async with db() as session:
        await session.execute(
            update(UserProfile)
            .where(UserProfile.user_id == user_id)
            .values(
                emotional_state=emotional_state,
                updated_at=datetime.now()
            )
        )
        await session.commit()


# ==========================================
# üéì LEARNING PREFERENCES
# ==========================================

async def _update_learning_preferences(user_id: int, learning_data: dict):
    """
    –û–±–Ω–æ–≤–∏—Ç—å learning preferences (—á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç/–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)
    """
    profile = await user_profile.get_or_create(user_id)
    learning_prefs = profile.learning_preferences
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ insights (–±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)
    works_well = set(learning_prefs.get('works_well', []))
    doesnt_work = set(learning_prefs.get('doesnt_work', []))
    
    works_well.update(learning_data.get('works_well', []))
    doesnt_work.update(learning_data.get('doesnt_work', []))
    
    # Limit: –ø–æ 10 –∫–∞–∂–¥–æ–≥–æ
    learning_prefs['works_well'] = list(works_well)[-10:]
    learning_prefs['doesnt_work'] = list(doesnt_work)[-10:]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    from database.database import db
    from sqlalchemy import update
    from database.models.user_profile import UserProfile
    
    async with db() as session:
        await session.execute(
            update(UserProfile)
            .where(UserProfile.user_id == user_id)
            .values(
                learning_preferences=learning_prefs,
                updated_at=datetime.now()
            )
        )
        await session.commit()


# ==========================================
# üéØ PUBLIC API
# ==========================================

async def analyze_if_needed(user_id: int, assistant_type: str = 'helper'):
    """
    –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω—É–∂–µ–Ω –ª–∏ –∞–Ω–∞–ª–∏–∑ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    
    –¢—Ä–∏–≥–≥–µ—Ä—ã:
    - –ü–æ—Å–ª–µ 3 —Å–æ–æ–±—â–µ–Ω–∏–π ‚Üí quick analysis (—É–≤–µ–ª–∏—á–µ–Ω–æ —Å 5 –¥–ª—è —Ä–æ—Å—Ç–∞ occurrences)
    - –ü–æ—Å–ª–µ 20 —Å–æ–æ–±—â–µ–Ω–∏–π ‚Üí deep analysis
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        assistant_type: –¢–∏–ø –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """
    if not is_feature_enabled('ENABLE_PATTERN_ANALYSIS'):
        return
    
    # –°—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
    message_count = await conversation_history.count_messages(user_id, assistant_type)
    
    # Quick analysis –∫–∞–∂–¥—ã–µ 3 —Å–æ–æ–±—â–µ–Ω–∏—è (–±—ã–ª–æ 5 - —É–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —Ä–æ—Å—Ç–∞ occurrences)
    if message_count > 0 and message_count % 3 == 0:
        await quick_analysis(user_id, assistant_type)
    
    # Deep analysis –∫–∞–∂–¥—ã–µ 20 —Å–æ–æ–±—â–µ–Ω–∏–π
    if message_count > 0 and message_count % 20 == 0:
        await deep_analysis(user_id, assistant_type)

