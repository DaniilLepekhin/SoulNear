"""
üß† Pattern Analyzer Service - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:
1. Quick Analysis (–ø–æ—Å–ª–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π) - –ø–∞—Ç—Ç–µ—Ä–Ω—ã + mood
2. Deep Analysis (–ø–æ—Å–ª–µ 20 —Å–æ–æ–±—â–µ–Ω–∏–π) - –∏–Ω—Å–∞–π—Ç—ã + recommendations
3. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ embeddings
4. Learning loop (—á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç/–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)

Architecture: Moderate + Embeddings
"""
import logging
import re
import uuid
from collections import OrderedDict
from datetime import datetime, timedelta
from typing import Optional
import json

from openai import AsyncOpenAI
from sqlalchemy import update

from config import OPENAI_API_KEY, is_feature_enabled
from bot.services import embedding_service
from bot.services.pattern_context_filter import (
    infer_context_weights_from_tags,
    merge_context_weights,
    normalize_topic,
)
from bot.services.constants import (
    SIMILARITY_THRESHOLD_DUPLICATE,
    SIMILARITY_THRESHOLD_RELATED,
    QUICK_ANALYSIS_FREQUENCY,
    DEEP_ANALYSIS_FREQUENCY,
    QUICK_ANALYSIS_CONTEXT_SIZE,
    DEEP_ANALYSIS_CONTEXT_SIZE,
    QUICK_ANALYSIS_MIN_MESSAGES,
    DEEP_ANALYSIS_MIN_MESSAGES,
    MAX_EVIDENCE_PER_PATTERN,
    MAX_INSIGHTS,
    MAX_LEARNING_ITEMS,
    MODEL_ANALYSIS,
    TEMPERATURE_ANALYSIS,
    BURNOUT_SCORE_THRESHOLD,    # üÜï V2.1
    DEPRESSION_SCORE_THRESHOLD,  # üÜï V2.1
    PATTERN_TITLE_TRANSLATIONS,
    ALLOWED_PATTERN_TITLES,
    PATTERN_TITLE_ASCII_REGEX,
)
from bot.services.prompt.analysis_prompts import get_quick_analysis_prompt, get_deep_analysis_prompt
from database.repository import user_profile, conversation_history
from database.database import db
from database.models.user_profile import UserProfile
import database.repository.user as db_user

logger = logging.getLogger(__name__)

client = AsyncOpenAI(api_key=OPENAI_API_KEY)


QUICK_ANALYSIS_RESPONSE_FORMAT = {
    "type": "json_schema",
    "json_schema": {
        "name": "quick_analysis_response",
        "schema": {
            "type": "object",
            "properties": {
                "new_patterns": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "type": {
                                "type": "string",
                                "enum": ["behavioral", "emotional", "cognitive"],
                            },
                            "title": {
                                "type": "string",
                                "pattern": PATTERN_TITLE_ASCII_REGEX,
                                "maxLength": 80,
                            },
                            "description": {"type": "string"},
                            "contradiction": {"type": "string"},
                            "hidden_dynamic": {"type": "string"},
                            "blocked_resource": {"type": "string"},
                            "evidence": {
                                "type": "array",
                                "items": {"type": "string"},
                                "minItems": 1,
                                "maxItems": 3,
                            },
                            "tags": {
                                "type": "array",
                                "items": {"type": "string"},
                                "maxItems": 5,
                            },
                            "primary_context": {
                                "type": "string",
                                "maxLength": 32
                            },
                            "context_weights": {
                                "type": "object",
                                "additionalProperties": {"type": "number"}
                            },
                            "frequency": {
                                "type": "string",
                                "enum": ["high", "medium", "low"],
                            },
                            "confidence": {
                                "type": "number",
                                "minimum": 0.0,
                                "maximum": 1.0,
                            },
                            "response_hint": {
                                "type": "string",
                                "maxLength": 400
                            }
                        },
                        "required": [
                            "type",
                            "title",
                            "description",
                            "contradiction",
                            "hidden_dynamic",
                            "blocked_resource",
                            "evidence",
                            "frequency",
                            "confidence",
                            "response_hint",
                        ],
                        "additionalProperties": True,
                    },
                },
                "mood": {
                    "type": "object",
                    "properties": {
                        "current_mood": {"type": "string"},
                        "stress_level": {
                            "type": "string",
                            "enum": ["low", "medium", "high", "critical"],
                        },
                        "energy_level": {
                            "type": "string",
                            "enum": ["low", "medium", "high"],
                        },
                        "triggers": {
                            "type": "array",
                            "items": {"type": "string"},
                            "maxItems": 5,
                        },
                    },
                    "required": ["current_mood", "stress_level", "energy_level"],
                    "additionalProperties": True,
                },
            },
            "required": ["new_patterns", "mood"],
            "additionalProperties": True,
        },
    },
}


# ==========================================
# üîß –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –£–¢–ò–õ–ò–¢–´
# ==========================================


def _contains_cyrillic(text: str | None) -> bool:
    if not text:
        return False
    return bool(re.search(r"[–ê-–Ø–∞-—è–Å—ë]", text))


def _normalize_new_patterns(patterns: list[dict] | None) -> list[dict]:
    """–ü—Ä–∏–≤–µ—Å—Ç–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ—Ç GPT –∫ –æ–∂–∏–¥–∞–µ–º–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É"""
    if not patterns:
        return []

    normalized: list[dict] = []
    for pattern in patterns:
        if not isinstance(pattern, dict):
            continue

        title = (pattern.get('title') or '').strip()
        if title in PATTERN_TITLE_TRANSLATIONS:
            translated = PATTERN_TITLE_TRANSLATIONS[title]
            if translated != title:
                logger.debug(f"üî§ Translated pattern title '{title}' ‚Üí '{translated}'")
            title = translated
        elif _contains_cyrillic(title):
            translated = PATTERN_TITLE_TRANSLATIONS.get(title)
            if translated:
                logger.debug(f"üî§ Translated pattern title '{title}' ‚Üí '{translated}'")
                title = translated

        if not title:
            title = "Pattern Insight"

        if title not in ALLOWED_PATTERN_TITLES and not re.fullmatch(PATTERN_TITLE_ASCII_REGEX, title):
            cleaned_title = re.sub(r"[^A-Za-z0-9 ,\-()']", "", title)
            if cleaned_title:
                logger.debug(f"üî§ Normalized pattern title '{title}' ‚Üí '{cleaned_title}'")
                title = cleaned_title
            else:
                title = "Pattern Insight"

        pattern['title'] = title

        evidence = pattern.get('evidence') or []
        if isinstance(evidence, list):
            # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å UI
            trimmed_evidence = [str(item).strip() for item in evidence if isinstance(item, str) and item.strip()]
            pattern['evidence'] = trimmed_evidence[:3]
        else:
            pattern['evidence'] = []

        tags = pattern.get('tags') or []
        if not isinstance(tags, list):
            tags = []
        pattern['tags'] = [str(tag) for tag in tags][:5]

        context_weights = pattern.get('context_weights')
        if isinstance(context_weights, dict) and context_weights:
            cleaned_weights: dict[str, float] = {}
            for raw_key, raw_value in context_weights.items():
                try:
                    cleaned_weights[normalize_topic(str(raw_key))] = float(raw_value)
                except (TypeError, ValueError):
                    continue
            pattern['context_weights'] = cleaned_weights
        else:
            pattern['context_weights'] = infer_context_weights_from_tags(pattern)

        primary_context = pattern.get('primary_context')
        if isinstance(primary_context, str) and primary_context.strip():
            normalized_primary = normalize_topic(primary_context)
            pattern['primary_context'] = normalized_primary
            if pattern['context_weights'] is not None:
                current = pattern['context_weights'].get(normalized_primary, 0.0)
                pattern['context_weights'][normalized_primary] = max(current, 1.0)
        else:
            pattern['primary_context'] = None

        response_hint = pattern.get('response_hint')
        if isinstance(response_hint, str):
            pattern['response_hint'] = response_hint.strip()
        else:
            pattern['response_hint'] = None

        normalized.append(pattern)

    return normalized


def _build_response_hint_from_pattern(pattern: dict) -> Optional[dict]:
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å hint –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞."""

    hint_text = (pattern.get('response_hint') or '').strip()
    if not hint_text:
        return None

    source = {
        'type': 'pattern',
        'title': pattern.get('title'),
        'frequency': pattern.get('frequency'),
    }
    if pattern.get('contradiction'):
        source['contradiction'] = pattern.get('contradiction')

    return {
        'hint': hint_text,
        'source': source,
    }


def _build_response_hint_from_insight(insight: dict) -> Optional[dict]:
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å hint –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å–∞–π—Ç–∞."""

    hint_text = (insight.get('response_hint') or '').strip()
    if not hint_text:
        return None

    source = {
        'type': 'insight',
        'title': insight.get('title'),
        'priority': insight.get('priority'),
    }
    if insight.get('category'):
        source['category'] = insight.get('category')

    return {
        'hint': hint_text,
        'source': source,
    }


# ==========================================
# üéØ QUICK ANALYSIS (–∫–∞–∂–¥—ã–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π)
# ==========================================

async def quick_analysis(user_id: int, assistant_type: str = 'helper'):
    """
    –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 5-10 —Å–æ–æ–±—â–µ–Ω–∏–π
    
    –í—ã—è–≤–ª—è–µ—Ç:
    - 1-2 –Ω–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    - –¢–µ–∫—É—â–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ (mood)
    - –£—Ä–æ–≤–µ–Ω—å —Å—Ç—Ä–µ—Å—Å–∞/—ç–Ω–µ—Ä–≥–∏–∏
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        assistant_type: –¢–∏–ø –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """
    if not is_feature_enabled('ENABLE_PATTERN_ANALYSIS'):
        return
    
    try:
        logger.info(f"Quick analysis for user {user_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–∏–∑ constants)
        messages = await conversation_history.get_context(
            user_id=user_id,
            assistant_type=assistant_type,
            max_messages=QUICK_ANALYSIS_CONTEXT_SIZE
        )
        
        if len(messages) < QUICK_ANALYSIS_MIN_MESSAGES:
            logger.debug("Not enough messages for analysis")
            return
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ñ–∏–ª—å
        profile = await user_profile.get_or_create(user_id)
        existing_patterns = profile.patterns.get('patterns', [])
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ GPT-4
        analysis = await _analyze_conversation_quick(messages, existing_patterns)
        
        if not analysis:
            logger.warning(f"[QUICK ANALYSIS] User {user_id}: GPT returned None")
            return
        
        # LOG: –°–∫–æ–ª—å–∫–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤–µ—Ä–Ω—É–ª GPT
        new_patterns_count = len(analysis.get('new_patterns', []))
        logger.info(f"[QUICK ANALYSIS] User {user_id}: GPT returned {new_patterns_count} new patterns")
        
        # ‚ö†Ô∏è –í–∞–ª–∏–¥–∞—Ü–∏—è: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ examples –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏–∑ —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏
        analyzed_patterns = analysis.get('new_patterns', []) if analysis else []

        if analyzed_patterns:
            validated_patterns = [
                _validate_pattern_examples(pattern, messages)
                for pattern in analyzed_patterns
            ]
            analysis['new_patterns'] = validated_patterns
            analyzed_patterns = validated_patterns
        
        # üö® SAFETY NET: –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏–ª –ª–∏ GPT –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        # (Option 4: Two-Stage Detection)
        critical_missing = _check_critical_patterns_missing(
            messages=messages,
            existing_patterns=(analyzed_patterns + existing_patterns)
        )
        
        if critical_missing:
            logger.warning(
                f"[QUICK ANALYSIS] User {user_id}: Safety net added {len(critical_missing)} "
                f"critical patterns that GPT missed"
            )
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫ –Ω–æ–≤—ã–º
            if not analysis.get('new_patterns'):
                analysis['new_patterns'] = []
            analysis['new_patterns'].extend(critical_missing)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã (—Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π)
        if analysis.get('new_patterns'):
            await _add_patterns_with_dedup(user_id, analysis['new_patterns'], existing_patterns)
            # LOG: –°–∫–æ–ª—å–∫–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ—Å–ª–µ –º–µ—Ä–¥–∂–∞
            updated_profile = await user_profile.get_or_create(user_id)
            total_patterns = len(updated_profile.patterns.get('patterns', []))
            logger.info(f"[QUICK ANALYSIS] User {user_id}: Total patterns after merge: {total_patterns}")

            pattern_hints = [
                _build_response_hint_from_pattern(pattern)
                for pattern in analysis['new_patterns']
            ]
            pattern_hints = [hint for hint in pattern_hints if hint]
            if pattern_hints:
                await user_profile.add_response_hints(user_id, pattern_hints)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º emotional state
        if analysis.get('mood'):
            mood_data = analysis['mood'].copy()
            
            # ‚ö†Ô∏è –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ stress level: –Ω–∞—à–∞ heuristic vs GPT
            # –ï—Å–ª–∏ –Ω–∞—à–∞ —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞—Ö–æ–¥–∏—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π —Å—Ç—Ä–µ—Å—Å - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
            updated_profile = await user_profile.get_or_create(user_id)
            all_patterns = updated_profile.patterns.get('patterns', [])
            calculated_stress = _calculate_stress_level(all_patterns, messages)
            gpt_stress = mood_data.get('stress_level', 'medium')
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: critical > high > medium > low
            stress_priority = {'critical': 4, 'high': 3, 'medium': 2, 'low': 1}
            if stress_priority.get(calculated_stress, 0) > stress_priority.get(gpt_stress, 0):
                logger.info(
                    f"‚ö†Ô∏è Stress level override: GPT={gpt_stress}, "
                    f"calculated={calculated_stress}. Using calculated value."
                )
                mood_data['stress_level'] = calculated_stress
            
            await _update_emotional_state(user_id, mood_data)
        
        logger.info(f"Quick analysis complete: {len(analysis.get('new_patterns', []))} patterns, mood={analysis.get('mood', {}).get('current_mood')}")
        
    except Exception as e:
        logger.error(f"Quick analysis failed: {e}")


async def _analyze_conversation_quick(
    messages: list[dict],
    existing_patterns: list[dict]
) -> Optional[dict]:
    """
    GPT-4 –∞–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–∞ (quick version)
    """
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞
    conversation_text = "\n".join([
        f"{msg['role']}: {msg['content']}" 
        for msg in messages[-10:]
    ])
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä existing patterns (GPT-4 –∫–æ–Ω—Ç–µ–∫—Å—Ç)
    existing_summaries = [
        f"- {p['title']}" 
        for p in existing_patterns[:10]
    ]
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –º–æ–¥—É–ª—è
    prompt = get_quick_analysis_prompt(conversation_text, existing_summaries)
    
    try:
        response = await client.chat.completions.create(
            model=MODEL_ANALYSIS,  # üÜï V2.1: Use constant (now gpt-4o for better depth)
            messages=[
                {"role": "system", "content": "You are an expert psychologist analyzing conversation patterns."},
                {"role": "user", "content": prompt}
            ],
            response_format=QUICK_ANALYSIS_RESPONSE_FORMAT,
            temperature=TEMPERATURE_ANALYSIS
        )
        
        result = json.loads(response.choices[0].message.content)
        result['new_patterns'] = _normalize_new_patterns(result.get('new_patterns'))
        
        # üÜï V2.1: Log GPT response –¥–ª—è debugging
        patterns_count = len(result.get('new_patterns', []))
        logger.info(f"‚úÖ GPT quick_analysis returned {patterns_count} patterns (model: {MODEL_ANALYSIS})")
        
        if patterns_count > 0:
            # Log first pattern title for visibility
            first_title = result['new_patterns'][0].get('title', 'N/A')
            logger.debug(f"üìã First pattern: '{first_title}'")
            
            # Log if V2 fields present
            has_v2 = any(
                'contradiction' in p or 'hidden_dynamic' in p or 'blocked_resource' in p
                for p in result['new_patterns']
            )
            if has_v2:
                logger.info("‚ú® V2 fields detected in patterns!")
            else:
                logger.warning("‚ö†Ô∏è V2 fields MISSING in patterns (GPT didn't return them)")
        
        return result
        
    except Exception as e:
        logger.error(f"GPT-4 analysis failed: {e}")
        return None


# ==========================================
# üîç DEEP ANALYSIS (–∫–∞–∂–¥—ã–µ 20 —Å–æ–æ–±—â–µ–Ω–∏–π)
# ==========================================

async def deep_analysis(user_id: int, assistant_type: str = 'helper'):
    """
    –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤
    
    –í—ã—è–≤–ª—è–µ—Ç:
    - –°–≤—è–∑–∏ –º–µ–∂–¥—É –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
    - –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
    - –ò–Ω—Å–∞–π—Ç—ã —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
    - –û–±–Ω–æ–≤–ª—è–µ—Ç learning preferences
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        assistant_type: –¢–∏–ø –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """
    if not is_feature_enabled('ENABLE_PATTERN_ANALYSIS'):
        return
    
    try:
        logger.info(f"Deep analysis for user {user_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–∏–∑ constants)
        messages = await conversation_history.get_context(
            user_id=user_id,
            assistant_type=assistant_type,
            max_messages=DEEP_ANALYSIS_CONTEXT_SIZE
        )
        
        if len(messages) < DEEP_ANALYSIS_MIN_MESSAGES:
            logger.debug("Not enough messages for deep analysis")
            return
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ñ–∏–ª—å
        profile = await user_profile.get_or_create(user_id)
        existing_patterns = profile.patterns.get('patterns', [])
        existing_insights = profile.insights.get('insights', [])
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ GPT-4
        analysis = await _analyze_conversation_deep(messages, existing_patterns, existing_insights)
        
        if not analysis:
            return
        
        # üö® SAFETY NET: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ quick_analysis –ø—Ä–æ–ø—É—Å—Ç–∏–ª)
        critical_missing = _check_critical_patterns_missing(
            messages=messages,
            existing_patterns=existing_patterns  # –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ—Ç–∏–≤ –í–°–ï–• –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤, –Ω–µ —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö
        )
        
        if critical_missing:
            logger.warning(
                f"[DEEP ANALYSIS] User {user_id}: Safety net found {len(critical_missing)} "
                f"missing critical patterns. Force-adding."
            )
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä—è–º–æ –≤ –ø—Ä–æ—Ñ–∏–ª—å (–º–∏–Ω—É—è GPT)
            await _add_patterns_with_dedup(user_id, critical_missing, existing_patterns)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω—Å–∞–π—Ç—ã
        if analysis.get('insights'):
            await _add_insights(user_id, analysis['insights'], existing_patterns)

            insight_hints = [
                _build_response_hint_from_insight(insight)
                for insight in analysis['insights']
            ]
            insight_hints = [hint for hint in insight_hints if hint]
            if insight_hints:
                await user_profile.add_response_hints(user_id, insight_hints)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º related patterns
        if existing_patterns:
            await _update_related_patterns(user_id, existing_patterns)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º learning preferences
        if analysis.get('learning'):
            await _update_learning_preferences(user_id, analysis['learning'])
        
        logger.info(f"Deep analysis complete: {len(analysis.get('insights', []))} insights generated")
        
    except Exception as e:
        logger.error(f"Deep analysis failed: {e}")


async def _analyze_conversation_deep(
    messages: list[dict],
    existing_patterns: list[dict],
    existing_insights: list[dict]
) -> Optional[dict]:
    """
    GPT-4 –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ (insights + recommendations)
    """
    conversation_text = "\n".join([
        f"{msg['role']}: {msg['content']}" 
        for msg in messages[-30:]
    ])
    
    patterns_summary = "\n".join([
        f"- [{p['type']}] {p['title']}: {p['description']} (occurs: {p.get('occurrences', 1)}x)"
        for p in existing_patterns[:15]
    ])
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –º–æ–¥—É–ª—è
    prompt = get_deep_analysis_prompt(conversation_text, patterns_summary)
    
    try:
        response = await client.chat.completions.create(
            model="gpt-4o",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—É—é –≤–µ—Ä—Å–∏—é –¥–ª—è deep analysis
            messages=[
                {"role": "system", "content": "You are an expert psychologist providing deep insights."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.4
        )
        
        result = json.loads(response.choices[0].message.content)
        return result
        
    except Exception as e:
        logger.error(f"GPT-4 deep analysis failed: {e}")
        return None


# ==========================================
# üîÑ –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø –ò –ú–ï–†–î–ñ –ü–ê–¢–¢–ï–†–ù–û–í
# ==========================================

def _validate_pattern_examples(pattern: dict, recent_messages: list[dict]) -> dict:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è —á—Ç–æ examples –≤ –ø–∞—Ç—Ç–µ—Ä–Ω–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö
    
    Args:
        pattern: –ü–∞—Ç—Ç–µ—Ä–Ω —Å –ø–æ–ª–µ–º 'evidence' (examples)
        recent_messages: –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
    Returns:
        –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω (—É–¥–∞–ª—è–µ—Ç –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ examples)
    """
    evidence = pattern.get('evidence', [])
    if not evidence:
        return pattern
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—Ç–æ–ª—å–∫–æ user, –Ω–µ assistant)
    user_messages_text = ' '.join([
        msg.get('content', '').lower()
        for msg in recent_messages
        if msg.get('role') == 'user'
    ])
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º evidence: –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ —á—Ç–æ –µ—Å—Ç—å –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö
    validated_evidence = []
    for example in evidence:
        example_lower = example.lower().strip()
        
        # –ï—Å–ª–∏ –ø—Ä–∏–º–µ—Ä –∫–æ—Ä–æ—Ç–∫–∏–π (< 5 —Å–∏–º–≤–æ–ª–æ–≤) - —Å–∫–∏–ø–∞–µ–º (—Å–ª–∏—à–∫–æ–º –æ–±—â–∏–π)
        if len(example_lower) < 5:
            logger.debug(f"‚ö†Ô∏è Example too short, skipping: '{example}'")
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø—Ä–∏–º–µ—Ä –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö (–∏–ª–∏ –µ–≥–æ —á–∞—Å—Ç—å >= 70%)
        if example_lower in user_messages_text:
            validated_evidence.append(example)
        else:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—Ö–æ—Ç—è –±—ã 70% —Å–ª–æ–≤)
            example_words = set(example_lower.split())
            if len(example_words) > 0:
                # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –∏–∑ example –µ—Å—Ç—å –≤ messages
                matched_words = sum(1 for word in example_words if word in user_messages_text)
                match_ratio = matched_words / len(example_words)
                
                if match_ratio >= 0.7:  # 70% —Å–ª–æ–≤ —Å–æ–≤–ø–∞–ª–æ
                    validated_evidence.append(example)
                    logger.debug(f"‚úÖ Partial match ({match_ratio:.0%}): '{example}'")
                else:
                    logger.warning(
                        f"‚ùå Example NOT found in messages (match: {match_ratio:.0%}): '{example}'. "
                        f"Pattern: {pattern.get('title')}"
                    )
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω
    pattern['evidence'] = validated_evidence
    pattern['validated_at'] = datetime.now().isoformat()
    
    # –ï—Å–ª–∏ –≤—Å–µ examples –±—ã–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã - –ø–æ–Ω–∏–∂–∞–µ–º confidence
    if len(evidence) > 0 and len(validated_evidence) == 0:
        old_confidence = pattern.get('confidence', 0.7)
        pattern['confidence'] = max(0.3, old_confidence * 0.5)
        logger.warning(
            f"‚ö†Ô∏è All examples invalid for pattern '{pattern.get('title')}'. "
            f"Confidence lowered: {old_confidence:.2f} ‚Üí {pattern['confidence']:.2f}"
        )
    
    return pattern


def _calculate_burnout_score(recent_text: str) -> int:
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç burnout score –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–∏–º–ø—Ç–æ–º–æ–≤
    
    Args:
        recent_text: –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (lowercase)
        
    Returns:
        Burnout score (0-20+)
    """
    import re
    
    score = 0
    
    # CRITICAL SYMPTOMS (3 points each):
    critical_symptoms = {
        'overwork': r'—Ä–∞–±–æ—Ç–∞\w* (–ø–æ )?\d+ —á–∞—Å',  # —Ä–∞–±–æ—Ç–∞—é 14 —á–∞—Å–æ–≤
        'cognitive_dysfunction': r'(–∑–∞–±—ã–ª|–≤—ã–ø–∞–ª–æ –∏–∑ –≥–æ–ª–æ–≤—ã).*(–≤–∞–∂–Ω|–≤—Å—Ç—Ä–µ—á|–¥–µ–¥–ª–∞–π–Ω|–∑–∞–¥–∞—á)',
        'concentration': r'–Ω–µ –º–æ–≥—É (—Å–∫–æ–Ω—Ü–µ–Ω—Ç—Ä|–∫–æ–Ω—Ü–µ–Ω—Ç—Ä|—Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á|–¥—É–º–∞—Ç—å)',  # fixed: added "—Å–∫–æ–Ω—Ü–µ–Ω—Ç—Ä"
        'anhedonia': r'–Ω–µ –ø–æ–º–Ω—é –∫–æ–≥–¥–∞.*(—Å—á–∞—Å—Ç–ª–∏–≤|—Ä–∞–¥–æ–≤–∞–ª|—É–¥–æ–≤–æ–ª—å—Å—Ç–≤)',
    }
    
    for symptom, pattern in critical_symptoms.items():
        if re.search(pattern, recent_text):
            score += 3
            logger.debug(f"üî• Burnout critical symptom detected: {symptom}")
    
    # MAJOR SYMPTOMS (2 points each):
    major_symptoms = [
        r'–Ω–µ—Ç —Å–∏–ª',
        r'—É—Å—Ç–∞–ª\w*',
        r'–≤—ã–≥–æ—Ä–∞–Ω\w*',
        r'–∫–∞–∫ —Ä–æ–±–æ—Ç',
        r'–Ω–∞ –∏–∑–Ω–æ—Å',
        r'–∫–∞–∂–¥—ã–π –¥–µ–Ω—å —Ä–∞–±–æ—Ç',
        r'–±–µ–∑ –≤—ã—Ö–æ–¥–Ω—ã—Ö',
        r'–Ω–µ –æ—Ç–¥—ã—Ö–∞–ª',
    ]
    
    for pattern in major_symptoms:
        if re.search(pattern, recent_text):
            score += 2
            logger.debug(f"üî• Burnout major symptom detected: {pattern}")
    
    # MINOR SYMPTOMS (1 point each):
    minor_symptoms = [
        r'–∑–∞—á–µ–º —Å—Ç–∞—Ä–∞—Ç—å—Å—è',
        r'–Ω–µ—Ç —Å–º—ã—Å–ª–∞',
        r'–≤—Å—ë –Ω–∞–¥–æ–µ–ª–æ',
        r'—Ö–æ—á–µ—Ç—Å—è –±—Ä–æ—Å–∏—Ç—å',
    ]
    
    for pattern in minor_symptoms:
        if re.search(pattern, recent_text):
            score += 1
    
    logger.info(f"üî• Burnout score calculated: {score}")
    return score


def _extract_burnout_evidence(messages: list[dict], max_evidence: int = 3) -> list[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç evidence –¥–ª—è burnout –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π
    
    Args:
        messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
        max_evidence: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ examples
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Ü–∏—Ç–∞—Ç (evidence)
    """
    import re
    
    evidence = []
    
    burnout_keywords = [
        r'—Ä–∞–±–æ—Ç–∞\w* (–ø–æ )?\d+ —á–∞—Å',
        r'–∑–∞–±—ã–ª.*(–≤—Å—Ç—Ä–µ—á|–¥–µ–¥–ª–∞–π–Ω)',
        r'–Ω–µ –º–æ–≥—É (—Å–∫–æ–Ω—Ü–µ–Ω—Ç—Ä|–∫–æ–Ω—Ü–µ–Ω—Ç—Ä|–¥—É–º–∞—Ç—å)',  # fixed: added "—Å–∫–æ–Ω—Ü–µ–Ω—Ç—Ä"
        r'–Ω–µ—Ç —Å–∏–ª',
        r'–Ω–µ –ø–æ–º–Ω—é –∫–æ–≥–¥–∞.*—Å—á–∞—Å—Ç–ª–∏–≤',
        r'–∫–∞–∫ —Ä–æ–±–æ—Ç',
        r'–≤—ã–≥–æ—Ä–∞–Ω',
    ]
    
    for msg in messages:
        if msg.get('role') != 'user':
            continue
        
        content = msg.get('content', '')
        content_lower = content.lower()
        
        for pattern in burnout_keywords:
            if re.search(pattern, content_lower) and content not in evidence:
                evidence.append(content)
                break
        
        if len(evidence) >= max_evidence:
            break
    
    return evidence


def _calculate_depression_score(recent_text: str) -> int:
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç depression score –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–∏–º–ø—Ç–æ–º–æ–≤
    
    Args:
        recent_text: –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (lowercase)
        
    Returns:
        Depression score (0-20+)
    """
    import re
    
    score = 0
    
    # CRITICAL SYMPTOMS (4 points each):
    critical_symptoms = {
        'suicidal_ideation': r'(—Ö–æ—á—É —É–º–µ—Ä–µ—Ç—å|—Ö–æ—á–µ—Ç—Å—è –∏—Å—á–µ–∑–Ω—É—Ç—å|—Å—É–∏—Ü–∏–¥|–ø–æ–∫–æ–Ω—á–∏—Ç—å —Å)',
        'severe_hopelessness': r'(–Ω–µ—Ç —Å–º—ã—Å–ª–∞ –∂–∏—Ç—å|–≤—Å—ë –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω–æ|–∑–∞—á–µ–º –∂–∏—Ç—å)',
    }
    
    for symptom, pattern in critical_symptoms.items():
        if re.search(pattern, recent_text):
            score += 4
            logger.warning(f"üö® CRITICAL depression symptom detected: {symptom}")
    
    # MAJOR SYMPTOMS (3 points each):
    major_symptoms = {
        'hopelessness': r'(–Ω–µ—Ç —Å–º—ã—Å–ª–∞|–∑–∞—á–µ–º —Å—Ç–∞—Ä–∞—Ç—å—Å—è|–≤—Å—ë –±–µ—Å–ø–æ–ª–µ–∑–Ω–æ|–Ω–µ –≤–∏–∂—É —Å–º—ã—Å–ª–∞|–∫–∞–∫–æ–π —Å–º—ã—Å–ª)',
        'anhedonia': r'–Ω–µ –ø–æ–º–Ω—é –∫–æ–≥–¥–∞.*(—Å—á–∞—Å—Ç–ª–∏–≤|—Ä–∞–¥–æ–≤–∞–ª|—É–¥–æ–≤–æ–ª—å—Å—Ç–≤)',
        'worthlessness': r'(–ª—É–∑–µ—Ä|–Ω–µ—É–¥–∞—á–Ω–∏–∫|–≤—Å—ë –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ|–Ω–µ–∫–æ–º–ø–µ—Ç–µ–Ω—Ç|–Ω–∏—á–µ–≥–æ –Ω–µ —Å—Ç–æ—é|–±–µ—Å–ø–æ–ª–µ–∑–Ω)',
        'no_way_out': r'(–Ω–µ –≤–∏–∂—É –≤—ã—Ö–æ–¥–∞|–Ω–µ—Ç –≤—ã—Ö–æ–¥–∞|–±–µ–∑–≤—ã—Ö–æ–¥–Ω)',  # üÜï V2.1: Added pattern
    }
    
    for symptom, pattern in major_symptoms.items():
        if re.search(pattern, recent_text):
            score += 3
            logger.debug(f"‚ö†Ô∏è Depression major symptom detected: {symptom}")
    
    # MINOR SYMPTOMS (1 point each):
    minor_symptoms = [
        r'–Ω–µ—Ç —Å–∏–ª',
        r'—É—Å—Ç–∞–ª\w* –æ—Ç –≤—Å–µ–≥–æ',
        r'–≤—Å—ë –Ω–∞–¥–æ–µ–ª–æ',
    ]
    
    for pattern in minor_symptoms:
        if re.search(pattern, recent_text):
            score += 1
    
    logger.info(f"‚ö†Ô∏è Depression score calculated: {score}")
    return score


def _extract_depression_evidence(messages: list[dict], max_evidence: int = 3) -> list[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç evidence –¥–ª—è depression –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π
    """
    import re
    
    evidence = []
    
    depression_keywords = [
        r'–Ω–µ—Ç —Å–º—ã—Å–ª–∞',
        r'–Ω–µ –≤–∏–∂—É —Å–º—ã—Å–ª–∞',
        r'–∑–∞—á–µ–º —Å—Ç–∞—Ä–∞—Ç—å—Å—è',
        r'–Ω–µ –ø–æ–º–Ω—é –∫–æ–≥–¥–∞.*—Å—á–∞—Å—Ç–ª–∏–≤',
        r'–Ω–µ –≤–∏–∂—É –≤—ã—Ö–æ–¥–∞',
        r'–ª—É–∑–µ—Ä',
        r'–≤—Å—ë –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ',
        r'–Ω–∏—á–µ–≥–æ –Ω–µ —Å—Ç–æ—é',
        r'—Ö–æ—á—É —É–º–µ—Ä–µ—Ç—å',
    ]
    
    for msg in messages:
        if msg.get('role') != 'user':
            continue
        
        content = msg.get('content', '')
        content_lower = content.lower()
        
        for pattern in depression_keywords:
            if re.search(pattern, content_lower) and content not in evidence:
                evidence.append(content)
                break
        
        if len(evidence) >= max_evidence:
            break
    
    return evidence


def _check_critical_patterns_missing(
    messages: list[dict],
    existing_patterns: list[dict]
) -> list[dict]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏–ª –ª–∏ GPT –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (burnout, depression)
    
    –≠—Ç–æ safety net: –µ—Å–ª–∏ GPT –Ω–µ —Å–æ–∑–¥–∞–ª –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω, –Ω–æ —Å–∏–º–ø—Ç–æ–º—ã –µ—Å—Ç—å,
    –º—ã force-add –µ–≥–æ –≤—Ä—É—á–Ω—É—é.
    
    Args:
        messages: –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        existing_patterns: –ü–∞—Ç—Ç–µ—Ä–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤–µ—Ä–Ω—É–ª GPT
        
    Returns:
        –°–ø–∏—Å–æ–∫ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å
    """
    missing = []
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 10 user messages
    recent_text = ' '.join([
        msg.get('content', '').lower()
        for msg in messages[-15:]
        if msg.get('role') == 'user'
    ])
    
    # CHECK 1: Burnout
    has_burnout = any(
        p.get('title', '').lower() in ['burnout', '–≤—ã–≥–æ—Ä–∞–Ω–∏–µ', 'professional burnout']
        for p in existing_patterns
    )
    
    if not has_burnout:
        burnout_score = _calculate_burnout_score(recent_text)
        
        # üÜï V2.1: Use constant threshold
        if burnout_score >= BURNOUT_SCORE_THRESHOLD:
            logger.warning(
                f"üö® GPT MISSED critical pattern: Burnout (score={burnout_score}). Force-adding."
            )
            
            evidence = _extract_burnout_evidence(messages)
            
            missing.append({
                'type': 'behavioral',
                'title': 'Burnout',
                'description': 'Professional burnout with cognitive dysfunction and emotional exhaustion',
                'evidence': evidence,
                'tags': ['critical', 'mental-health', 'auto-detected'],
                'frequency': 'high',
                'confidence': min(1.0, 0.7 + (burnout_score / 30)),  # 0.7-1.0
                'auto_detected': True,
                'detection_score': burnout_score
            })
    
    # CHECK 2: Acute Depression
    has_depression = any(
        'depression' in p.get('title', '').lower() or '–¥–µ–ø—Ä–µ—Å—Å' in p.get('title', '').lower()
        for p in existing_patterns
    )
    
    if not has_depression:
        depression_score = _calculate_depression_score(recent_text)
        logger.info(
            "‚ö†Ô∏è Safety net depression score: %s (threshold: %s)",
            depression_score,
            DEPRESSION_SCORE_THRESHOLD,
        )
        
        # üÜï V2.1: Use constant threshold (lowered from 9 to 7 for better detection)
        if depression_score >= DEPRESSION_SCORE_THRESHOLD:
            logger.warning(
                f"üö® GPT MISSED critical pattern: Acute Depression (score={depression_score}). Force-adding."
            )
            
            evidence = _extract_depression_evidence(messages)
            
            missing.append({
                'type': 'emotional',
                'title': 'Acute Depression',
                'description': 'Severe depressive symptoms requiring professional attention',
                'evidence': evidence,
                'tags': ['critical', 'mental-health', 'auto-detected', 'seek-help'],
                'frequency': 'high',
                'confidence': min(1.0, 0.7 + (depression_score / 30)),
                'auto_detected': True,
                'detection_score': depression_score,
                'requires_professional_help': True
            })
    
    if missing:
        logger.info(f"‚úÖ Safety net added {len(missing)} missing critical patterns")
    
    return missing


async def _add_patterns_with_dedup(
    user_id: int,
    new_patterns: list[dict],
    existing_patterns: list[dict]
):
    """
    –î–æ–±–∞–≤–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã —á–µ—Ä–µ–∑ embeddings
    
    –î–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω—ã–π –º–µ—Ä–¥–∂:
    1. Keyword match (exact title) ‚Üí —Ñ–æ—Ä—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ä–¥–∂ (100% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
    2. Semantic similarity (embedding > threshold) ‚Üí –æ–±—ã—á–Ω—ã–π –º–µ—Ä–¥–∂
    """
    for new_pattern in new_patterns:
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è embedding
            pattern_text = f"{new_pattern['title']} {new_pattern['description']}"
            
            # FACTOR 1: Keyword match (exact title)
            keyword_match = None
            for existing in existing_patterns:
                if existing['title'].lower().strip() == new_pattern['title'].lower().strip():
                    keyword_match = existing
                    break
            
            # FACTOR 2: Semantic similarity
            is_dup, duplicate, similarity = await embedding_service.is_duplicate(
                pattern_text,
                existing_patterns,
                text_key='description',
                threshold=SIMILARITY_THRESHOLD_DUPLICATE
            )
            
            # –î–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ: keyword match > semantic similarity
            if keyword_match:
                # FORCED MERGE –ø–æ keyword (100% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
                duplicate = keyword_match
                is_dup = True
                similarity = 1.0
                logger.info(f"üîë KEYWORD MATCH: '{new_pattern['title']}' ‚Üí forced merge")
            
            if is_dup:
                # –ú–µ—Ä–¥–∂–∏–º —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º
                old_occurrences = duplicate.get('occurrences', 1)
                duplicate['occurrences'] = old_occurrences + 1
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ evidence (–±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –º–∞–∫—Å–∏–º—É–º 10)
                existing_evidence = set(duplicate.get('evidence', []))
                new_evidence = [e for e in new_pattern.get('evidence', []) if e not in existing_evidence]
                duplicate['evidence'].extend(new_evidence)
                duplicate['evidence'] = duplicate['evidence'][-10:]  # Limit to last 10
                
                duplicate['last_detected'] = datetime.now().isoformat()
                duplicate['confidence'] = max(duplicate['confidence'], new_pattern.get('confidence', 0.7))
                
                # üÜï V2: Update deep analysis fields (always take latest from GPT)
                if 'contradiction' in new_pattern:
                    duplicate['contradiction'] = new_pattern['contradiction']
                if 'hidden_dynamic' in new_pattern:
                    duplicate['hidden_dynamic'] = new_pattern['hidden_dynamic']
                if 'blocked_resource' in new_pattern:
                    duplicate['blocked_resource'] = new_pattern['blocked_resource']
                # Also update description if GPT refined it
                if 'description' in new_pattern and new_pattern['description']:
                    duplicate['description'] = new_pattern['description']

                duplicate['context_weights'] = merge_context_weights(
                    duplicate.get('context_weights', {}),
                    new_pattern.get('context_weights', {}),
                )

                new_primary = new_pattern.get('primary_context')
                if new_primary:
                    duplicate['primary_context'] = normalize_topic(new_primary)
                
                logger.info(f"‚úÖ MERGED: '{new_pattern['title']}' ‚Üí '{duplicate['title']}' | similarity: {similarity:.2f} | occurrences: {old_occurrences} ‚Üí {duplicate['occurrences']}")
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π
                new_pattern['id'] = str(uuid.uuid4())
                new_pattern['first_detected'] = datetime.now().isoformat()
                new_pattern['last_detected'] = datetime.now().isoformat()
                new_pattern['occurrences'] = 1
                new_pattern['related_patterns'] = []
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding
                new_pattern['embedding'] = await embedding_service.get_embedding(pattern_text)
                
                existing_patterns.append(new_pattern)
                
                logger.info(f"Added new pattern: {new_pattern['title']}")
        
        except Exception as e:
            logger.error(f"Failed to process pattern: {e}")
            continue
    
    # Limit: –º–∞–∫—Å–∏–º—É–º 20 –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (—É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å –Ω–∏–∑–∫–∏–º confidence)
    if len(existing_patterns) > 20:
        existing_patterns.sort(
            key=lambda x: (x.get('confidence', 0.5), x.get('occurrences', 1)),
            reverse=True
        )
        existing_patterns = existing_patterns[:20]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    await user_profile.update_patterns(user_id, existing_patterns)


async def _update_related_patterns(user_id: int, patterns: list[dict]):
    """
    –û–±–Ω–æ–≤–∏—Ç—å related_patterns —á–µ—Ä–µ–∑ semantic similarity
    """
    for i, pattern in enumerate(patterns):
        if 'embedding' not in pattern or not pattern['embedding']:
            continue
        
        # –ù–∞—Ö–æ–¥–∏–º 3 –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞ (–∏—Å–∫–ª—é—á–∞—è —Å–µ–±—è)
        other_patterns = [p for j, p in enumerate(patterns) if j != i]
        
        related = await embedding_service.find_similar_items(
            pattern['embedding'],
            other_patterns,
            threshold=SIMILARITY_THRESHOLD_RELATED,
            top_k=3
        )
        
        pattern['related_patterns'] = [p['id'] for p, _ in related]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    await user_profile.update_patterns(user_id, patterns)


# ==========================================
# üí° –ò–ù–°–ê–ô–¢–´
# ==========================================

async def _add_insights(user_id: int, new_insights: list[dict], existing_patterns: list[dict]):
    """
    –î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Å–∞–π—Ç—ã (—Å —Å–≤—è–∑—å—é –∫ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º)
    """
    profile = await user_profile.get_or_create(user_id)
    existing_insights = profile.insights.get('insights', [])
    
    for insight in new_insights:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID
        insight['id'] = str(uuid.uuid4())
        insight['created_at'] = datetime.now().isoformat()
        insight['last_updated'] = datetime.now().isoformat()
        
        # –°–≤—è–∑—ã–≤–∞–µ–º —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ (–ø–æ title)
        derived_titles = insight.get('derived_from_pattern_titles', [])
        insight['derived_from'] = [
            p['id'] for p in existing_patterns
            if p['title'] in derived_titles
        ]
        
        existing_insights.append(insight)
    
    # Limit: –º–∞–∫—Å–∏–º—É–º 10 –∏–Ω—Å–∞–π—Ç–æ–≤
    if len(existing_insights) > 10:
        existing_insights = existing_insights[-10:]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    await user_profile.update_insights(user_id, existing_insights)


# ==========================================
# üòä EMOTIONAL STATE
# ==========================================

def _calculate_stress_level(patterns: list[dict], recent_messages: list[dict]) -> str:
    """
    –†–∞—Å—Å—á–∏—Ç–∞—Ç—å —É—Ä–æ–≤–µ–Ω—å —Å—Ç—Ä–µ—Å—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ —Å–æ–æ–±—â–µ–Ω–∏–π
    
    Uses:
    1. Pattern-based scoring (burnout, panic, etc.)
    2. Direct burnout score calculation from messages
    
    Args:
        patterns: –°–ø–∏—Å–æ–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        recent_messages: –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        
    Returns:
        'critical' | 'high' | 'medium' | 'low'
    """
    stress_score = 0
    
    # 1. PATTERN-BASED SCORING
    stress_patterns = ['burnout', 'exhaustion', '–≤—ã–≥–æ—Ä–∞–Ω–∏–µ', '–ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞']
    critical_patterns = ['panic', '–ø–∞–Ω–∏–∫–∞', 'despair', '–æ—Ç—á–∞—è–Ω–∏–µ', 'depression', '–¥–µ–ø—Ä–µ—Å—Å']
    
    for pattern in patterns:
        title_lower = pattern.get('title', '').lower()
        category_lower = pattern.get('category', '').lower()
        frequency = pattern.get('occurrences', 1)
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (panic, despair, acute depression)
        if any(kw in title_lower or kw in category_lower for kw in critical_patterns):
            stress_score += 4 * frequency
        
        # Burnout —Å –≤—ã—Å–æ–∫–æ–π —á–∞—Å—Ç–æ—Ç–æ–π
        if any(kw in title_lower or kw in category_lower for kw in stress_patterns):
            stress_score += 3 * frequency
        
        # –°—Ç—Ä–∞—Ö –Ω–µ—É–¥–∞—á–∏, —Å–∏–Ω–¥—Ä–æ–º —Å–∞–º–æ–∑–≤–∞–Ω—Ü–∞
        if any(kw in title_lower for kw in ['—Å—Ç—Ä–∞—Ö', '—Å–∞–º–æ–∑–≤–∞–Ω–µ—Ü', 'fear', 'imposter']):
            stress_score += 2 * frequency
    
    # 2. DIRECT BURNOUT SCORE from messages (more accurate)
    recent_text = ' '.join([
        msg.get('content', '').lower() 
        for msg in recent_messages[-10:]
        if msg.get('role') == 'user'
    ])
    
    burnout_score = _calculate_burnout_score(recent_text)
    
    # If burnout score is high, it contributes directly to stress
    # Scaling: burnout_score 6-12 ‚Üí stress +6-12
    stress_score += burnout_score
    
    # 3. EXPLICIT CHECK: If burnout detected ‚Üí force high/critical stress
    if burnout_score >= 6:  # 2+ critical burnout symptoms
        logger.warning(
            f"üö® High burnout score detected ({burnout_score}). "
            f"Forcing stress_level >= 'high'"
        )
        # Ensure stress is at least 'high' if burnout present
        if stress_score < 6:
            stress_score = 6
    
    # 4. CLASSIFICATION (UPDATED THRESHOLDS - more aggressive)
    if stress_score >= 10:
        return 'critical'  # –±—ã–ª–æ 15
    elif stress_score >= 6:
        return 'high'      # –±—ã–ª–æ 8
    elif stress_score >= 2:
        return 'medium'    # –±—ã–ª–æ 3
    else:
        return 'low'


def _detect_contradictions(emotional_state: dict, patterns: list[dict]) -> dict:
    """
    –û–±–Ω–∞—Ä—É–∂–∏—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –º–µ–∂–¥—É emotional_state –∏ patterns
    
    Args:
        emotional_state: –¢–µ–∫—É—â–µ–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        patterns: –°–ø–∏—Å–æ–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        
    Returns:
        –°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å —Ñ–ª–∞–≥–æ–º 'auto_corrected'
    """
    corrected_state = emotional_state.copy()
    auto_corrections = []
    
    stress_level = emotional_state.get('stress_level', 'medium')
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É burnout/stress –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    stress_frequency = sum(
        p.get('occurrences', 1) 
        for p in patterns 
        if any(kw in p.get('title', '').lower() or kw in p.get('category', '').lower()
               for kw in ['burnout', '–≤—ã–≥–æ—Ä–∞–Ω–∏–µ', 'stress', '—Å—Ç—Ä–µ—Å—Å', 'exhaustion'])
    )
    
    # –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ: stress_level='low' –Ω–æ –≤—ã—Å–æ–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞ stress patterns
    if stress_level in ['low', 'medium'] and stress_frequency >= 5:
        logger.warning(
            f"‚ö†Ô∏è Contradiction detected: stress_level={stress_level}, "
            f"but burnout patterns frequency={stress_frequency}. Auto-correcting to 'high'."
        )
        corrected_state['stress_level'] = 'high'
        auto_corrections.append({
            'field': 'stress_level',
            'old_value': stress_level,
            'new_value': 'high',
            'reason': f'High burnout pattern frequency ({stress_frequency})'
        })
    
    # –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ: energy_level='high' –Ω–æ –º–Ω–æ–≥–æ —É—Å—Ç–∞–ª–æ—Å—Ç–∏ –≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö
    energy_level = emotional_state.get('energy_level', 'medium')
    fatigue_frequency = sum(
        p.get('occurrences', 1)
        for p in patterns
        if any(kw in p.get('title', '').lower()
               for kw in ['—É—Å—Ç–∞–ª–æ—Å—Ç—å', '–Ω–µ—Ç —Å–∏–ª', 'exhaustion', 'fatigue', 'burnout'])
    )
    
    if energy_level == 'high' and fatigue_frequency >= 3:
        logger.warning(
            f"‚ö†Ô∏è Contradiction detected: energy_level={energy_level}, "
            f"but fatigue patterns frequency={fatigue_frequency}. Auto-correcting to 'low'."
        )
        corrected_state['energy_level'] = 'low'
        auto_corrections.append({
            'field': 'energy_level',
            'old_value': energy_level,
            'new_value': 'low',
            'reason': f'High fatigue pattern frequency ({fatigue_frequency})'
        })
    
    if auto_corrections:
        corrected_state['auto_corrections'] = auto_corrections
        corrected_state['last_correction'] = datetime.now().isoformat()
    
    return corrected_state


async def _update_emotional_state(user_id: int, mood_data: dict):
    """
    –û–±–Ω–æ–≤–∏—Ç—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    """
    profile = await user_profile.get_or_create(user_id)
    emotional_state = profile.emotional_state
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    emotional_state['current_mood'] = mood_data.get('current_mood', 'neutral')
    emotional_state['stress_level'] = mood_data.get('stress_level', 'medium')
    emotional_state['energy_level'] = mood_data.get('energy_level', 'medium')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é (limit: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π)
    mood_history = emotional_state.get('mood_history', [])
    today = datetime.now().date().isoformat()
    
    # –£–¥–∞–ª—è–µ–º —Å–µ–≥–æ–¥–Ω—è—à–Ω—é—é –∑–∞–ø–∏—Å—å –µ—Å–ª–∏ –µ—Å—Ç—å (–±—É–¥–µ–º –æ–±–Ω–æ–≤–ª—è—Ç—å)
    mood_history = [m for m in mood_history if m.get('date') != today]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é
    mood_history.append({
        'date': today,
        'mood': mood_data.get('current_mood'),
        'triggers': mood_data.get('triggers', [])
    })
    
    # Limit: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –∑–∞–ø–∏—Å–µ–π
    emotional_state['mood_history'] = mood_history[-30:]
    
    # ‚ö†Ô∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π: –µ—Å–ª–∏ GPT —Å–∫–∞–∑–∞–ª "stress=low", –Ω–æ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≥–æ–≤–æ—Ä—è—Ç "burnout"
    patterns = profile.patterns.get('patterns', [])
    emotional_state = _detect_contradictions(emotional_state, patterns)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    async with db() as session:
        await session.execute(
            update(UserProfile)
            .where(UserProfile.user_id == user_id)
            .values(
                emotional_state=emotional_state,
                updated_at=datetime.now()
            )
        )
        await session.commit()


# ==========================================
# üéì LEARNING PREFERENCES
# ==========================================

async def _update_learning_preferences(user_id: int, learning_data: dict):
    """
    –û–±–Ω–æ–≤–∏—Ç—å learning preferences (—á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç/–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç OrderedDict –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞ (–Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –∫–æ–Ω–µ—Ü).
    –≠—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è UI - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–µ—Ä–≤—ã–º–∏.
    """
    profile = await user_profile.get_or_create(user_id)
    learning_prefs = profile.learning_preferences
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º OrderedDict –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞ (–Ω–æ–≤—ã–µ –≤ –∫–æ–Ω–µ—Ü)
    # –ö–ª—é—á–∏ = items, –∑–Ω–∞—á–µ–Ω–∏—è = None (–Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–∏)
    works_well = OrderedDict.fromkeys(learning_prefs.get('works_well', []))
    doesnt_work = OrderedDict.fromkeys(learning_prefs.get('doesnt_work', []))
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (–¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è)
    for item in learning_data.get('works_well', []):
        works_well[item] = None
    for item in learning_data.get('doesnt_work', []):
        doesnt_work[item] = None
    
    # Limit: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 (—Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ)
    learning_prefs['works_well'] = list(works_well.keys())[-MAX_LEARNING_ITEMS:]
    learning_prefs['doesnt_work'] = list(doesnt_work.keys())[-MAX_LEARNING_ITEMS:]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    async with db() as session:
        await session.execute(
            update(UserProfile)
            .where(UserProfile.user_id == user_id)
            .values(
                learning_preferences=learning_prefs,
                updated_at=datetime.now()
            )
        )
        await session.commit()


# ==========================================
# üéØ PUBLIC API
# ==========================================

async def analyze_if_needed(user_id: int, assistant_type: str = 'helper'):
    """
    –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω—É–∂–µ–Ω –ª–∏ –∞–Ω–∞–ª–∏–∑ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    
    –¢—Ä–∏–≥–≥–µ—Ä—ã:
    - –ü–æ—Å–ª–µ 3 —Å–æ–æ–±—â–µ–Ω–∏–π ‚Üí quick analysis (—É–≤–µ–ª–∏—á–µ–Ω–æ —Å 5 –¥–ª—è —Ä–æ—Å—Ç–∞ occurrences)
    - –ü–æ—Å–ª–µ 20 —Å–æ–æ–±—â–µ–Ω–∏–π ‚Üí deep analysis
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        assistant_type: –¢–∏–ø –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """
    if not is_feature_enabled('ENABLE_PATTERN_ANALYSIS'):
        return
    
    # –°—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
    message_count = await conversation_history.count_messages(user_id, assistant_type)
    
    # Quick analysis (—á–∞—Å—Ç–æ—Ç–∞ –∏–∑ constants)
    if message_count > 0 and message_count % QUICK_ANALYSIS_FREQUENCY == 0:
        await quick_analysis(user_id, assistant_type)
    
    # Deep analysis (—á–∞—Å—Ç–æ—Ç–∞ –∏–∑ constants)
    if message_count > 0 and message_count % DEEP_ANALYSIS_FREQUENCY == 0:
        await deep_analysis(user_id, assistant_type)

