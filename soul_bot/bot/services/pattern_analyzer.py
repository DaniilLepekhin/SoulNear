"""
üß† Pattern Analyzer Service - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:
1. Quick Analysis (–ø–æ—Å–ª–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π) - –ø–∞—Ç—Ç–µ—Ä–Ω—ã + mood
2. Deep Analysis (–ø–æ—Å–ª–µ 20 —Å–æ–æ–±—â–µ–Ω–∏–π) - –∏–Ω—Å–∞–π—Ç—ã + recommendations
3. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ embeddings
4. Learning loop (—á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç/–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)

Architecture: Moderate + Embeddings
"""
import logging
import uuid
from collections import OrderedDict
from datetime import datetime, timedelta
from typing import Optional
import json

from openai import AsyncOpenAI
from sqlalchemy import update

from config import OPENAI_API_KEY, is_feature_enabled
from bot.services import embedding_service
from bot.services.constants import (
    SIMILARITY_THRESHOLD_DUPLICATE,
    SIMILARITY_THRESHOLD_RELATED,
    QUICK_ANALYSIS_FREQUENCY,
    DEEP_ANALYSIS_FREQUENCY,
    QUICK_ANALYSIS_CONTEXT_SIZE,
    DEEP_ANALYSIS_CONTEXT_SIZE,
    QUICK_ANALYSIS_MIN_MESSAGES,
    DEEP_ANALYSIS_MIN_MESSAGES,
    MAX_EVIDENCE_PER_PATTERN,
    MAX_INSIGHTS,
    MAX_LEARNING_ITEMS,
    MODEL_ANALYSIS,
    TEMPERATURE_ANALYSIS
)
from bot.services.prompt.analysis_prompts import get_quick_analysis_prompt, get_deep_analysis_prompt
from database.repository import user_profile, conversation_history
from database.database import db
from database.models.user_profile import UserProfile
import database.repository.user as db_user

logger = logging.getLogger(__name__)

client = AsyncOpenAI(api_key=OPENAI_API_KEY)


# ==========================================
# üéØ QUICK ANALYSIS (–∫–∞–∂–¥—ã–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π)
# ==========================================

async def quick_analysis(user_id: int, assistant_type: str = 'helper'):
    """
    –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 5-10 —Å–æ–æ–±—â–µ–Ω–∏–π
    
    –í—ã—è–≤–ª—è–µ—Ç:
    - 1-2 –Ω–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    - –¢–µ–∫—É—â–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ (mood)
    - –£—Ä–æ–≤–µ–Ω—å —Å—Ç—Ä–µ—Å—Å–∞/—ç–Ω–µ—Ä–≥–∏–∏
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        assistant_type: –¢–∏–ø –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """
    if not is_feature_enabled('ENABLE_PATTERN_ANALYSIS'):
        return
    
    try:
        logger.info(f"Quick analysis for user {user_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–∏–∑ constants)
        messages = await conversation_history.get_context(
            user_id=user_id,
            assistant_type=assistant_type,
            max_messages=QUICK_ANALYSIS_CONTEXT_SIZE
        )
        
        if len(messages) < QUICK_ANALYSIS_MIN_MESSAGES:
            logger.debug("Not enough messages for analysis")
            return
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ñ–∏–ª—å
        profile = await user_profile.get_or_create(user_id)
        existing_patterns = profile.patterns.get('patterns', [])
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ GPT-4
        analysis = await _analyze_conversation_quick(messages, existing_patterns)
        
        if not analysis:
            logger.warning(f"[QUICK ANALYSIS] User {user_id}: GPT returned None")
            return
        
        # LOG: –°–∫–æ–ª—å–∫–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤–µ—Ä–Ω—É–ª GPT
        new_patterns_count = len(analysis.get('new_patterns', []))
        logger.info(f"[QUICK ANALYSIS] User {user_id}: GPT returned {new_patterns_count} new patterns")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã (—Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π)
        if analysis.get('new_patterns'):
            await _add_patterns_with_dedup(user_id, analysis['new_patterns'], existing_patterns)
            # LOG: –°–∫–æ–ª—å–∫–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ—Å–ª–µ –º–µ—Ä–¥–∂–∞
            updated_profile = await user_profile.get_or_create(user_id)
            total_patterns = len(updated_profile.patterns.get('patterns', []))
            logger.info(f"[QUICK ANALYSIS] User {user_id}: Total patterns after merge: {total_patterns}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º emotional state
        if analysis.get('mood'):
            await _update_emotional_state(user_id, analysis['mood'])
        
        logger.info(f"Quick analysis complete: {len(analysis.get('new_patterns', []))} patterns, mood={analysis.get('mood', {}).get('current_mood')}")
        
    except Exception as e:
        logger.error(f"Quick analysis failed: {e}")


async def _analyze_conversation_quick(
    messages: list[dict],
    existing_patterns: list[dict]
) -> Optional[dict]:
    """
    GPT-4 –∞–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–∞ (quick version)
    """
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞
    conversation_text = "\n".join([
        f"{msg['role']}: {msg['content']}" 
        for msg in messages[-10:]
    ])
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä existing patterns (GPT-4 –∫–æ–Ω—Ç–µ–∫—Å—Ç)
    existing_summaries = [
        f"- {p['title']}" 
        for p in existing_patterns[:10]
    ]
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –º–æ–¥—É–ª—è
    prompt = get_quick_analysis_prompt(conversation_text, existing_summaries)
    
    try:
        response = await client.chat.completions.create(
            model="gpt-4o-mini",  # –î–µ—à–µ–≤–ª–µ –¥–ª—è quick analysis
            messages=[
                {"role": "system", "content": "You are an expert psychologist analyzing conversation patterns."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.3
        )
        
        result = json.loads(response.choices[0].message.content)
        return result
        
    except Exception as e:
        logger.error(f"GPT-4 analysis failed: {e}")
        return None


# ==========================================
# üîç DEEP ANALYSIS (–∫–∞–∂–¥—ã–µ 20 —Å–æ–æ–±—â–µ–Ω–∏–π)
# ==========================================

async def deep_analysis(user_id: int, assistant_type: str = 'helper'):
    """
    –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤
    
    –í—ã—è–≤–ª—è–µ—Ç:
    - –°–≤—è–∑–∏ –º–µ–∂–¥—É –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
    - –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
    - –ò–Ω—Å–∞–π—Ç—ã —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
    - –û–±–Ω–æ–≤–ª—è–µ—Ç learning preferences
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        assistant_type: –¢–∏–ø –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """
    if not is_feature_enabled('ENABLE_PATTERN_ANALYSIS'):
        return
    
    try:
        logger.info(f"Deep analysis for user {user_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–∏–∑ constants)
        messages = await conversation_history.get_context(
            user_id=user_id,
            assistant_type=assistant_type,
            max_messages=DEEP_ANALYSIS_CONTEXT_SIZE
        )
        
        if len(messages) < DEEP_ANALYSIS_MIN_MESSAGES:
            logger.debug("Not enough messages for deep analysis")
            return
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ñ–∏–ª—å
        profile = await user_profile.get_or_create(user_id)
        existing_patterns = profile.patterns.get('patterns', [])
        existing_insights = profile.insights.get('insights', [])
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ GPT-4
        analysis = await _analyze_conversation_deep(messages, existing_patterns, existing_insights)
        
        if not analysis:
            return
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω—Å–∞–π—Ç—ã
        if analysis.get('insights'):
            await _add_insights(user_id, analysis['insights'], existing_patterns)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º related patterns
        if existing_patterns:
            await _update_related_patterns(user_id, existing_patterns)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º learning preferences
        if analysis.get('learning'):
            await _update_learning_preferences(user_id, analysis['learning'])
        
        logger.info(f"Deep analysis complete: {len(analysis.get('insights', []))} insights generated")
        
    except Exception as e:
        logger.error(f"Deep analysis failed: {e}")


async def _analyze_conversation_deep(
    messages: list[dict],
    existing_patterns: list[dict],
    existing_insights: list[dict]
) -> Optional[dict]:
    """
    GPT-4 –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ (insights + recommendations)
    """
    conversation_text = "\n".join([
        f"{msg['role']}: {msg['content']}" 
        for msg in messages[-30:]
    ])
    
    patterns_summary = "\n".join([
        f"- [{p['type']}] {p['title']}: {p['description']} (occurs: {p.get('occurrences', 1)}x)"
        for p in existing_patterns[:15]
    ])
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –º–æ–¥—É–ª—è
    prompt = get_deep_analysis_prompt(conversation_text, patterns_summary)
    
    try:
        response = await client.chat.completions.create(
            model="gpt-4o",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—É—é –≤–µ—Ä—Å–∏—é –¥–ª—è deep analysis
            messages=[
                {"role": "system", "content": "You are an expert psychologist providing deep insights."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.4
        )
        
        result = json.loads(response.choices[0].message.content)
        return result
        
    except Exception as e:
        logger.error(f"GPT-4 deep analysis failed: {e}")
        return None


# ==========================================
# üîÑ –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø –ò –ú–ï–†–î–ñ –ü–ê–¢–¢–ï–†–ù–û–í
# ==========================================

async def _add_patterns_with_dedup(
    user_id: int,
    new_patterns: list[dict],
    existing_patterns: list[dict]
):
    """
    –î–æ–±–∞–≤–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã —á–µ—Ä–µ–∑ embeddings
    
    –î–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω—ã–π –º–µ—Ä–¥–∂:
    1. Keyword match (exact title) ‚Üí —Ñ–æ—Ä—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ä–¥–∂ (100% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
    2. Semantic similarity (embedding > threshold) ‚Üí –æ–±—ã—á–Ω—ã–π –º–µ—Ä–¥–∂
    """
    for new_pattern in new_patterns:
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è embedding
            pattern_text = f"{new_pattern['title']} {new_pattern['description']}"
            
            # FACTOR 1: Keyword match (exact title)
            keyword_match = None
            for existing in existing_patterns:
                if existing['title'].lower().strip() == new_pattern['title'].lower().strip():
                    keyword_match = existing
                    break
            
            # FACTOR 2: Semantic similarity
            is_dup, duplicate, similarity = await embedding_service.is_duplicate(
                pattern_text,
                existing_patterns,
                text_key='description',
                threshold=SIMILARITY_THRESHOLD_DUPLICATE
            )
            
            # –î–≤—É—Ö—Ñ–∞–∫—Ç–æ—Ä–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ: keyword match > semantic similarity
            if keyword_match:
                # FORCED MERGE –ø–æ keyword (100% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
                duplicate = keyword_match
                is_dup = True
                similarity = 1.0
                logger.info(f"üîë KEYWORD MATCH: '{new_pattern['title']}' ‚Üí forced merge")
            
            if is_dup:
                # –ú–µ—Ä–¥–∂–∏–º —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º
                old_occurrences = duplicate.get('occurrences', 1)
                duplicate['occurrences'] = old_occurrences + 1
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ evidence (–±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –º–∞–∫—Å–∏–º—É–º 10)
                existing_evidence = set(duplicate.get('evidence', []))
                new_evidence = [e for e in new_pattern.get('evidence', []) if e not in existing_evidence]
                duplicate['evidence'].extend(new_evidence)
                duplicate['evidence'] = duplicate['evidence'][-10:]  # Limit to last 10
                
                duplicate['last_detected'] = datetime.now().isoformat()
                duplicate['confidence'] = max(duplicate['confidence'], new_pattern.get('confidence', 0.7))
                
                logger.info(f"‚úÖ MERGED: '{new_pattern['title']}' ‚Üí '{duplicate['title']}' | similarity: {similarity:.2f} | occurrences: {old_occurrences} ‚Üí {duplicate['occurrences']}")
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π
                new_pattern['id'] = str(uuid.uuid4())
                new_pattern['first_detected'] = datetime.now().isoformat()
                new_pattern['last_detected'] = datetime.now().isoformat()
                new_pattern['occurrences'] = 1
                new_pattern['related_patterns'] = []
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding
                new_pattern['embedding'] = await embedding_service.get_embedding(pattern_text)
                
                existing_patterns.append(new_pattern)
                
                logger.info(f"Added new pattern: {new_pattern['title']}")
        
        except Exception as e:
            logger.error(f"Failed to process pattern: {e}")
            continue
    
    # Limit: –º–∞–∫—Å–∏–º—É–º 20 –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (—É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å –Ω–∏–∑–∫–∏–º confidence)
    if len(existing_patterns) > 20:
        existing_patterns.sort(
            key=lambda x: (x.get('confidence', 0.5), x.get('occurrences', 1)),
            reverse=True
        )
        existing_patterns = existing_patterns[:20]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    await user_profile.update_patterns(user_id, existing_patterns)


async def _update_related_patterns(user_id: int, patterns: list[dict]):
    """
    –û–±–Ω–æ–≤–∏—Ç—å related_patterns —á–µ—Ä–µ–∑ semantic similarity
    """
    for i, pattern in enumerate(patterns):
        if 'embedding' not in pattern or not pattern['embedding']:
            continue
        
        # –ù–∞—Ö–æ–¥–∏–º 3 –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞ (–∏—Å–∫–ª—é—á–∞—è —Å–µ–±—è)
        other_patterns = [p for j, p in enumerate(patterns) if j != i]
        
        related = await embedding_service.find_similar_items(
            pattern['embedding'],
            other_patterns,
            threshold=SIMILARITY_THRESHOLD_RELATED,
            top_k=3
        )
        
        pattern['related_patterns'] = [p['id'] for p, _ in related]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    await user_profile.update_patterns(user_id, patterns)


# ==========================================
# üí° –ò–ù–°–ê–ô–¢–´
# ==========================================

async def _add_insights(user_id: int, new_insights: list[dict], existing_patterns: list[dict]):
    """
    –î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Å–∞–π—Ç—ã (—Å —Å–≤—è–∑—å—é –∫ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º)
    """
    profile = await user_profile.get_or_create(user_id)
    existing_insights = profile.insights.get('insights', [])
    
    for insight in new_insights:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID
        insight['id'] = str(uuid.uuid4())
        insight['created_at'] = datetime.now().isoformat()
        insight['last_updated'] = datetime.now().isoformat()
        
        # –°–≤—è–∑—ã–≤–∞–µ–º —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ (–ø–æ title)
        derived_titles = insight.get('derived_from_pattern_titles', [])
        insight['derived_from'] = [
            p['id'] for p in existing_patterns
            if p['title'] in derived_titles
        ]
        
        existing_insights.append(insight)
    
    # Limit: –º–∞–∫—Å–∏–º—É–º 10 –∏–Ω—Å–∞–π—Ç–æ–≤
    if len(existing_insights) > 10:
        existing_insights = existing_insights[-10:]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    await user_profile.update_insights(user_id, existing_insights)


# ==========================================
# üòä EMOTIONAL STATE
# ==========================================

async def _update_emotional_state(user_id: int, mood_data: dict):
    """
    –û–±–Ω–æ–≤–∏—Ç—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    """
    profile = await user_profile.get_or_create(user_id)
    emotional_state = profile.emotional_state
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    emotional_state['current_mood'] = mood_data.get('current_mood', 'neutral')
    emotional_state['stress_level'] = mood_data.get('stress_level', 'medium')
    emotional_state['energy_level'] = mood_data.get('energy_level', 'medium')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é (limit: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π)
    mood_history = emotional_state.get('mood_history', [])
    today = datetime.now().date().isoformat()
    
    # –£–¥–∞–ª—è–µ–º —Å–µ–≥–æ–¥–Ω—è—à–Ω—é—é –∑–∞–ø–∏—Å—å –µ—Å–ª–∏ –µ—Å—Ç—å (–±—É–¥–µ–º –æ–±–Ω–æ–≤–ª—è—Ç—å)
    mood_history = [m for m in mood_history if m.get('date') != today]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é
    mood_history.append({
        'date': today,
        'mood': mood_data.get('current_mood'),
        'triggers': mood_data.get('triggers', [])
    })
    
    # Limit: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –∑–∞–ø–∏—Å–µ–π
    emotional_state['mood_history'] = mood_history[-30:]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    async with db() as session:
        await session.execute(
            update(UserProfile)
            .where(UserProfile.user_id == user_id)
            .values(
                emotional_state=emotional_state,
                updated_at=datetime.now()
            )
        )
        await session.commit()


# ==========================================
# üéì LEARNING PREFERENCES
# ==========================================

async def _update_learning_preferences(user_id: int, learning_data: dict):
    """
    –û–±–Ω–æ–≤–∏—Ç—å learning preferences (—á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç/–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç OrderedDict –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞ (–Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –∫–æ–Ω–µ—Ü).
    –≠—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è UI - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–µ—Ä–≤—ã–º–∏.
    """
    profile = await user_profile.get_or_create(user_id)
    learning_prefs = profile.learning_preferences
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º OrderedDict –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞ (–Ω–æ–≤—ã–µ –≤ –∫–æ–Ω–µ—Ü)
    # –ö–ª—é—á–∏ = items, –∑–Ω–∞—á–µ–Ω–∏—è = None (–Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–∏)
    works_well = OrderedDict.fromkeys(learning_prefs.get('works_well', []))
    doesnt_work = OrderedDict.fromkeys(learning_prefs.get('doesnt_work', []))
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (–¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è)
    for item in learning_data.get('works_well', []):
        works_well[item] = None
    for item in learning_data.get('doesnt_work', []):
        doesnt_work[item] = None
    
    # Limit: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 (—Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ)
    learning_prefs['works_well'] = list(works_well.keys())[-MAX_LEARNING_ITEMS:]
    learning_prefs['doesnt_work'] = list(doesnt_work.keys())[-MAX_LEARNING_ITEMS:]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    async with db() as session:
        await session.execute(
            update(UserProfile)
            .where(UserProfile.user_id == user_id)
            .values(
                learning_preferences=learning_prefs,
                updated_at=datetime.now()
            )
        )
        await session.commit()


# ==========================================
# üéØ PUBLIC API
# ==========================================

async def analyze_if_needed(user_id: int, assistant_type: str = 'helper'):
    """
    –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω—É–∂–µ–Ω –ª–∏ –∞–Ω–∞–ª–∏–∑ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    
    –¢—Ä–∏–≥–≥–µ—Ä—ã:
    - –ü–æ—Å–ª–µ 3 —Å–æ–æ–±—â–µ–Ω–∏–π ‚Üí quick analysis (—É–≤–µ–ª–∏—á–µ–Ω–æ —Å 5 –¥–ª—è —Ä–æ—Å—Ç–∞ occurrences)
    - –ü–æ—Å–ª–µ 20 —Å–æ–æ–±—â–µ–Ω–∏–π ‚Üí deep analysis
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        assistant_type: –¢–∏–ø –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """
    if not is_feature_enabled('ENABLE_PATTERN_ANALYSIS'):
        return
    
    # –°—á–∏—Ç–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
    message_count = await conversation_history.count_messages(user_id, assistant_type)
    
    # Quick analysis (—á–∞—Å—Ç–æ—Ç–∞ –∏–∑ constants)
    if message_count > 0 and message_count % QUICK_ANALYSIS_FREQUENCY == 0:
        await quick_analysis(user_id, assistant_type)
    
    # Deep analysis (—á–∞—Å—Ç–æ—Ç–∞ –∏–∑ constants)
    if message_count > 0 and message_count % DEEP_ANALYSIS_FREQUENCY == 0:
        await deep_analysis(user_id, assistant_type)

